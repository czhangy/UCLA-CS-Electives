# COM SCI M146 - Fall '22 - Chang

[TOC]

## Lecture 1: Introduction

- What is Machine Learning?
  - Machine learning is the study of algorithms that improve on task `T` with respect to performance `P`, based on experience `E`
    - Example:
      - `T`: Recognizing handwritten words
      - `P`: Percentage of words correctly classified
      - `E`: Database of human-labeled images of handwritten words
  - A well-defined learning task is given by `<P, T, E>`
- Prerequisites
  - The pillars of machine learning
    - Probability and statistics
    - Linear algebra
    - Calculus/optimization
  - Computer science background
    - Algorithms
    - Programming experience
      - We will use Python and `scikit-learn`
- Goals of this Course
  - Fundamental concepts and algorithms
    - Customize your own algorithm
  - Common techniques/tools used
    - Theoretical understanding
    - Practical implementation
    - Best practices
  - How to "debug" an ML system
  - Black magic => systematic process
  - What will we learn?
    - Supervised learning
      - Decision tree, Perceptron, linear models, support vector machines, kernel methods, probabilistic models
    - Unsupervised learning
      - Clustering, EM algorithms
    - Learning theory
    - Deep learning (representation learning)
    - Practical issues
      - Experimental evaluation, implementing ML models
- Current Status
  - Compelling results on benchmarks
  - Works well in the general domain
  - Commercial uses
  - Challenges:
    - Incorporate with human knowledge
    - Reliable (fair, robust, interpretable) models that earn human trust
    - Applications in specific domains
    - Skewness of available annotation data
- Machine Learning is Interdisciplinary
  - Makes use of probability and statistics, linear algebra, calculus, theory of computation
  - Related to philosophy, psychology, neurobiology, linguistics, vision, robotics, etc.
  - Has applications in AI (natural language, vision, planning, HCI), engineering (agriculture, civil, etc.), and computer science (compilers, architecture, systems, databases, etc.)



## Lecture 2: Learning and Challenges in ML

- Learning Protocols

  - In general, we have an item `x` drawn from an input space `X` and an item `y` drawn from an output space `Y`
    - We're considering systems that apply a function `f()` to input items `x` and returns an output `y = f(x)`
  - Note that the boundary between supervised learning and unsupervised learning is blurry
  - Supervised Learning
    - Collects labeled data during the training phase
      - Ex) Pictures that are lions and pictures that aren't lions 
    - Performs the correct task during the test phase
    - In supervised learning, we have an item `x` drawn from an instance space `X` and an item `y` drawn from an label space `Y`, connected by a learned model that approximates this system, `y = g(x)`
    - A learning algorithm can be applied to the raw test data, generating a learned model `g(x)`
    - Some labeled data is reserved for testing, where test labels, `Y`, can be compared to the predicted labels, `g(X)`, generated by the learned model for evaluation
      - Analogous to a graded exam, where the student's answers are the predicted labels and the professor's answers are the test labels
  - Unsupervised Learning
    - Given: unlabeled, raw inputs
    - Goal: learn some intrinsic structure in inputs
      - Seems unpromising, but humans do it all the time
      - Ex) Clustering of minions/monsters, deciphering "nice to meet you"
  - Reinforcement Learning
    - Given sequences of states and actions with rewards
    - Learn policy that maximizes agent's reward
    - Analogous to training an animal with positive reinforcement

- Challenges in ML

  - Structured Inference
    - Sometimes, you need context to process data correctly
  - Robustness
    - Machines may struggle with some distinctions that humans can understand
    - Ex) Car vs. shoe
  - Adversarial Attack
    - Small perturbations may cause machines to label incorrectly, even if they make no difference to humans
    - Ex) Self-driving cars identifying speed limits
  - Common Sense
    - Humans have common sense, which allows conclusions to be drawn, even when information is missing
    - Machines have no such common sense
      - Winograd Schema (1972)
        - "The city councilmen refused the demonstrators a permit because they feared violence" vs. "The city councilmen refused the demonstrators a permit because they advocated violence"
        - Who is "they" referring to? => requires reasoning through common sense
      - Visual common sense
        - Ex) It's raining in an image since people are holding umbrellas => machines will search for the raindrops themselves and can't make the connection between umbrellas and rain
  - Fairness/Inclusion in ML
    - If a system works well 99% of the time, but fails for a specific group of people, it can cause problems
    - Complete representation is needed during the machine's training
    - Word Embedding Bias
      - Vectors can be used to associate words to genders
        - Ex) Man => uncle while woman => aunt
      - Runs into problems by capturing stereotypes
        - Ex) Man => doctor while woman => nurse
      - Language generations can be gendered
        - Ex) Misgendering in NLG

- Framing a Learning Problem

  - The Badges Game

    - An example of supervised learning
    - Conference attendees to the 1994 ML conference were given name badges labeled with + or -
    - What function was used to assign these labels?
      - If we saw another set of people, we should know how to assign +s and -s
      - If the second letter was a vowel => +, otherwise => -
      - In general, pick the simpler rule if multiple are possible

  - Using Supervised Learning

    - What is our instance space?

      - What kind of features are we using?
      - `x` is represented in a feature space
        - Typically `x ∈ {0, 1}^n` or `x ∈ R^n`
          - Boolean or real number
        - Usually represented as a vector, called an input vector
          - `X` is an `N`-dimensional vector space (e.g., `R^N`)
            - Each dimension = one feature
          - Each `x` is a feature vector
          - Think of `x = [x1, ..., xN]` as a point in `X`
        - Features can be human-defined, and the machine is tasked with searching through the feature space to find relevant ones
      - Ex) Badges game
        - Possible features: length of first/last name, contains letter "x", number of vowels in name, etc.
      - Good features are essential
        - The choice of features is crucial for how well a task can be learned
          - In many application areas (language, vision, etc.), a lot of work goes into designing suitable features
          - This requires domain expertise
        - This class won't cover what specific features to use for your task, but will touch on some general principles

    - What is our label space?

      - What kind of learning task are we dealing with?
      - `y` is represented in output/label space
      - Different kinds of output:
        - Binary classification: `y ∈ {-1, 1}`
          - Ex) Is this a lion?
        - Multiclass classification: `y ∈ {1, 2, 3, ..., K}`
          - Can be reduced into a binary classification
          - Ex) Is this a lion, cat, or dog?
        - Regression: `y ∈ R`
        - Structured output: `y ∈ {1, 2, 3, ..., K}^N`
          - Not covered in this course
          - A multilabel output (type of structured output) may have multiple answers
            - Ex) Is this a lion, cat, dog, or mammal?
      - In the context of this course, we will assume the features/output type are known

    - What is our hypothesis space?

      - What kind of functions (models) are we learning?

      - The machine can try many functions quickly, but doesn't have the intuition that we do to guide that search

      - We need to choose what *kind* of model we want to learn

      - A function `g` is consistent to a dataset:

        - $$
          D=\{(x_i,y_i)\}\text{ if }g(x_i)=y_i,\forall i
          $$

      - How many possible functions exist over 4 binary inputs?

        - Complete ignorance: there are `2^16` possible functions

    - What learning algorithm do we use?

      - How do we learn the model from the labeled data?

    - What is our loss function/evaluation metric?

      - How do we measure success? What drives learning?
      - Can be used by the model as a signal to improve



## Lecture 3: Hypothesis Space & KNN

- Using Supervised Learning (cont.)

  - What is our hypothesis space?

    - How many possible functions are consistent with the training data? (7 known inputs, 4 inputs)

      - `2^9` possible functions
      - Is learning possible?

    - The number of possible functions `f(x)` from the instance space `X` to the label space `Y` is:

      - $$
        |Y|^{|X|}
        $$

    - Learners typically consider only a subset of the functions from `X` to `Y`, called the hypothesis space `H`:

      - $$
        H\subseteq|Y|^{|X|}
        $$

    - Simple rules: conjunctive rules of the form:

      - $$
        y=x_i\land x_j\land\ ...\land\ x_k
        $$

    - `m`-of-`n` rules: if and only if at least `m` of the following `n` variables are `1`

- Views of Learning

  - Learning is the removal of our remaining uncertainty	
    - Ruling out of inconsistent functions in the hypothesis space using training data
  - Learning requires guessing a good hypothesis class
    - If it's too restrictive, you may find no consistent functions; if it's too general, you may find many consistent functions
    - Start with a small class and enlarge it until it contains a hypothesis that fits the data
  - We could be wrong!
    - Our guess of the hypothesis space could be wrong
    - It may even be possible that the hypothesis is consistent with the training data, but we're still off

- General Strategies for Machine Learning

  - Develop flexible hypothesis spaces
    - Decision trees, neural networks, nested collections, etc.
  - Develop algorithms for finding the "best" hypothesis in the hypothesis space that fits the data
    - Also, hope that it will generalize well

- Hypothesis Space – Real Value Features

  - Strong assumption that the data given in the training set will follow a given pattern => leveraged to create the hypothesis set
  - Underfitting and Overfitting
    - Goal is to find a hypothesis that is consistent with the training data, but without being too specific
    - Underfitting: the hypothesis doesn't fit the training data well
    - Overfitting: the hypothesis tries too hard to fit the training data, preventing actual learning and generalization

- Bias vs. Variance

  - Remember, training data are subsamples drawn from the true distribution
  - Example: Studying Strategy
    - Study every chapter well => low variance and low bias
    - Study only a few chapters => high variance and low bias
    - Study every chapter roughly => low variance and high bias
    - Don't study => high variance and high bias

- Overfitting the Data

  - A classifier performing perfectly on the training data may not lead to the best generalized performance
    - There may be noise in the training data
    - The algorithm might be making decisions based on very little data
  - As the complexity of the hypothesis increases, the accuracy on the training data will continue to increase, but the generalized performance will eventually drop due to overfitting
  - Preventing Overfitting
    - Using a less expressive model
      - e.g., linear model
    - Adding regularization
      - Promote simpler models
    - Data perturbation (add noise in training)
      - Can be done algorithmically (e.g., dropout)
      - May increase the bias of the model
    - Stop the optimization process earlier
      - Sounds bad in theory, but works in practice

- How Do We Learn?

  - How can we find a good model from the hypothesis space?

- Linear Functions

  - Challenges
    - The hypothesis space contains an infinite number of functions
    - Several functions are consistent with the data
  - A possibility: local search
    - Start with a linear threshold function
    - See how well you're doing
    - Correct
    - Repeat until you converge
    - Optimize a function with calculus

- K-Nearest Neighbor

  - Motivation:

    - Spam
    - Learning from memorization

  - Nearest Neighbors: The Basic Version

    - Training examples are vectors `xi` associated with a label `yi`
      - e.g., `xi` = a feature vector for an email, `yi` = spam
    - Learning: just store all the training examples
    - Prediction: for a new example `x`
      - Find the training example `xi` that is closest to `x`
      - Predict the label of `x` to the label `yi` associated with `xi`

  - K-Nearest Neighbors

    - Training examples are vectors `xi` associated with a label `yi`

      - e.g., `xi` = a feature vector for an email, `yi` = spam

    - Learning: just store all the training examples

    - Prediction: for a new example `x`

      - Find the `k` closest training examples to `x`

    - Construct the label of `x` using these `k` points

    - Issue: how do we define distance?

      - How do we measure distances between instances in vector space?

        - Euclidean distance:

          - $$
            ||x_1-x_2||_2=\sqrt{\sum_{i=1}^n\left(x_{1,i}-x_{2,i}\right)^2}
            $$

        - Manhattan distance:

          - $$
            ||x_1-x_2||_1=\sum_{i=1}^n\left|x_{1,i}-x_{2,i}\right|
            $$

        - `Lp`-norm

          - Euclidean = `L2`, Manhattan = `L1`

          - $$
            ||x_1-x_2||_p=\left(\sum_{i=1}^n\left|x_{1,i}-x_{2,i}\right|^p\right)^{\frac{1}{p}},\quad p>0
            $$

      - In general, a good place to inject knowledge about the domain

      - Behavior of this approach can depend on this

  - Distance Between Instances

    - Most common distance is the Hamming distance
      - Number of bits that are different, or number of features that have a different value



## Lecture 4:

- Learning Objectives:
  - KNN algorithms
  - Hyper-parameter tuning
    - Train/develop/test
    - `N`-fold cross-validation
  - Decision boundary
  - Curse of dimensionality
  - Practical concerns – data preprocessing
- KNN Algorithm
  - Training examples are vectors `xi` associated with a label `yi`
  - Learning: just store all the training examples
  - Prediction for a new example `x`:
    - Find the `k` closest training examples to `x`
    - Construct the label of `x` using these `k` points
  - 

