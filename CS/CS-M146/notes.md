# COM SCI M146 - Fall '22 - Chang

[TOC]

## Lecture 1: Introduction

- What is Machine Learning?
  - Machine learning is the study of algorithms that improve on task `T` with respect to performance `P`, based on experience `E`
    - Example:
      - `T`: Recognizing handwritten words
      - `P`: Percentage of words correctly classified
      - `E`: Database of human-labeled images of handwritten words
  - A well-defined learning task is given by `<P, T, E>`
- Prerequisites
  - The pillars of machine learning
    - Probability and statistics
    - Linear algebra
    - Calculus/optimization
  - Computer science background
    - Algorithms
    - Programming experience
      - We will use Python and `scikit-learn`
- Goals of this Course
  - Fundamental concepts and algorithms
    - Customize your own algorithm
  - Common techniques/tools used
    - Theoretical understanding
    - Practical implementation
    - Best practices
  - How to "debug" an ML system
  - Black magic => systematic process
  - What will we learn?
    - Supervised learning
      - Decision tree, Perceptron, linear models, support vector machines, kernel methods, probabilistic models
    - Unsupervised learning
      - Clustering, EM algorithms
    - Learning theory
    - Deep learning (representation learning)
    - Practical issues
      - Experimental evaluation, implementing ML models
- Current Status
  - Compelling results on benchmarks
  - Works well in the general domain
  - Commercial uses
  - Challenges:
    - Incorporate with human knowledge
    - Reliable (fair, robust, interpretable) models that earn human trust
    - Applications in specific domains
    - Skewness of available annotation data
- Machine Learning is Interdisciplinary
  - Makes use of probability and statistics, linear algebra, calculus, theory of computation
  - Related to philosophy, psychology, neurobiology, linguistics, vision, robotics, etc.
  - Has applications in AI (natural language, vision, planning, HCI), engineering (agriculture, civil, etc.), and computer science (compilers, architecture, systems, databases, etc.)



## Lecture 2: Learning and Challenges in ML

- Learning Protocols

  - In general, we have an item `x` drawn from an input space `X` and an item `y` drawn from an output space `Y`
    - We're considering systems that apply a function `f()` to input items `x` and returns an output `y = f(x)`
  - Note that the boundary between supervised learning and unsupervised learning is blurry
  - Supervised Learning
    - Collects labeled data during the training phase
      - Ex) Pictures that are lions and pictures that aren't lions 
    - Performs the correct task during the test phase
    - In supervised learning, we have an item `x` drawn from an instance space `X` and an item `y` drawn from an label space `Y`, connected by a learned model that approximates this system, `y = g(x)`
    - A learning algorithm can be applied to the raw test data, generating a learned model `g(x)`
    - Some labeled data is reserved for testing, where test labels, `Y`, can be compared to the predicted labels, `g(X)`, generated by the learned model for evaluation
      - Analogous to a graded exam, where the student's answers are the predicted labels and the professor's answers are the test labels
  - Unsupervised Learning
    - Given: unlabeled, raw inputs
    - Goal: learn some intrinsic structure in inputs
      - Seems unpromising, but humans do it all the time
      - Ex) Clustering of minions/monsters, deciphering "nice to meet you"
  - Reinforcement Learning
    - Given sequences of states and actions with rewards
    - Learn policy that maximizes agent's reward
    - Analogous to training an animal with positive reinforcement

- Challenges in ML

  - Structured Inference
    - Sometimes, you need context to process data correctly
  - Robustness
    - Machines may struggle with some distinctions that humans can understand
    - Ex) Car vs. shoe
  - Adversarial Attack
    - Small perturbations may cause machines to label incorrectly, even if they make no difference to humans
    - Ex) Self-driving cars identifying speed limits
  - Common Sense
    - Humans have common sense, which allows conclusions to be drawn, even when information is missing
    - Machines have no such common sense
      - Winograd Schema (1972)
        - "The city councilmen refused the demonstrators a permit because they feared violence" vs. "The city councilmen refused the demonstrators a permit because they advocated violence"
        - Who is "they" referring to? => requires reasoning through common sense
      - Visual common sense
        - Ex) It's raining in an image since people are holding umbrellas => machines will search for the raindrops themselves and can't make the connection between umbrellas and rain
  - Fairness/Inclusion in ML
    - If a system works well 99% of the time, but fails for a specific group of people, it can cause problems
    - Complete representation is needed during the machine's training
    - Word Embedding Bias
      - Vectors can be used to associate words to genders
        - Ex) Man => uncle while woman => aunt
      - Runs into problems by capturing stereotypes
        - Ex) Man => doctor while woman => nurse
      - Language generations can be gendered
        - Ex) Misgendering in NLG

- Framing a Learning Problem

  - The Badges Game

    - An example of supervised learning
    - Conference attendees to the 1994 ML conference were given name badges labeled with + or -
    - What function was used to assign these labels?
      - If we saw another set of people, we should know how to assign +s and -s
      - If the second letter was a vowel => +, otherwise => -
      - In general, pick the simpler rule if multiple are possible

  - Using Supervised Learning

    - What is our instance space?

      - What kind of features are we using?
      - `x` is represented in a feature space
        - Typically `x ∈ {0, 1}^n` or `x ∈ R^n`
          - Boolean or real number
        - Usually represented as a vector, called an input vector
          - `X` is an `N`-dimensional vector space (e.g., `R^N`)
            - Each dimension = one feature
          - Each `x` is a feature vector
          - Think of `x = [x1, ..., xN]` as a point in `X`
        - Features can be human-defined, and the machine is tasked with searching through the feature space to find relevant ones
      - Ex) Badges game
        - Possible features: length of first/last name, contains letter "x", number of vowels in name, etc.
      - Good features are essential
        - The choice of features is crucial for how well a task can be learned
          - In many application areas (language, vision, etc.), a lot of work goes into designing suitable features
          - This requires domain expertise
        - This class won't cover what specific features to use for your task, but will touch on some general principles

    - What is our label space?

      - What kind of learning task are we dealing with?
      - `y` is represented in output/label space
      - Different kinds of output:
        - Binary classification: `y ∈ {-1, 1}`
          - Ex) Is this a lion?
        - Multiclass classification: `y ∈ {1, 2, 3, ..., K}`
          - Can be reduced into a binary classification
          - Ex) Is this a lion, cat, or dog?
        - Regression: `y ∈ R`
        - Structured output: `y ∈ {1, 2, 3, ..., K}^N`
          - Not covered in this course
          - A multilabel output (type of structured output) may have multiple answers
            - Ex) Is this a lion, cat, dog, or mammal?
      - In the context of this course, we will assume the features/output type are known

    - What is our hypothesis space?

      - What kind of functions (models) are we learning?

      - The machine can try many functions quickly, but doesn't have the intuition that we do to guide that search

      - We need to choose what *kind* of model we want to learn

      - A function `g` is consistent to a dataset:

        - $$
          D=\{(x_i,y_i)\}\text{ if }g(x_i)=y_i,\forall i
          $$

      - How many possible functions exist over 4 binary inputs?

        - Complete ignorance: there are `2^16` possible functions

    - What learning algorithm do we use?

      - How do we learn the model from the labeled data?

    - What is our loss function/evaluation metric?

      - How do we measure success? What drives learning?
      - Can be used by the model as a signal to improve



## Lecture 3: Hypothesis Space & KNN

- Using Supervised Learning (cont.)

  - What is our hypothesis space?

    - How many possible functions are consistent with the training data? (7 known inputs, 4 inputs)

      - `2^9` possible functions
      - Is learning possible?

    - The number of possible functions `f(x)` from the instance space `X` to the label space `Y` is:

      - $$
        |Y|^{|X|}
        $$

    - Learners typically consider only a subset of the functions from `X` to `Y`, called the hypothesis space `H`:

      - $$
        H\subseteq|Y|^{|X|}
        $$

    - Simple rules: conjunctive rules of the form:

      - $$
        y=x_i\land x_j\land\ ...\land\ x_k
        $$

    - `m`-of-`n` rules: if and only if at least `m` of the following `n` variables are `1`

- Views of Learning

  - Learning is the removal of our remaining uncertainty	
    - Ruling out of inconsistent functions in the hypothesis space using training data
  - Learning requires guessing a good hypothesis class
    - If it's too restrictive, you may find no consistent functions; if it's too general, you may find many consistent functions
    - Start with a small class and enlarge it until it contains a hypothesis that fits the data
  - We could be wrong!
    - Our guess of the hypothesis space could be wrong
    - It may even be possible that the hypothesis is consistent with the training data, but we're still off

- General Strategies for Machine Learning

  - Develop flexible hypothesis spaces
    - Decision trees, neural networks, nested collections, etc.
  - Develop algorithms for finding the "best" hypothesis in the hypothesis space that fits the data
    - Also, hope that it will generalize well

- Hypothesis Space – Real Value Features

  - Strong assumption that the data given in the training set will follow a given pattern => leveraged to create the hypothesis set
  - Underfitting and Overfitting
    - Goal is to find a hypothesis that is consistent with the training data, but without being too specific
    - Underfitting: the hypothesis doesn't fit the training data well
    - Overfitting: the hypothesis tries too hard to fit the training data, preventing actual learning and generalization

- Bias vs. Variance

  - Remember, training data are subsamples drawn from the true distribution
  - Example: Studying Strategy
    - Study every chapter well => low variance and low bias
    - Study only a few chapters => high variance and low bias
    - Study every chapter roughly => low variance and high bias
    - Don't study => high variance and high bias

- Overfitting the Data

  - A classifier performing perfectly on the training data may not lead to the best generalized performance
    - There may be noise in the training data
    - The algorithm might be making decisions based on very little data
  - As the complexity of the hypothesis increases, the accuracy on the training data will continue to increase, but the generalized performance will eventually drop due to overfitting
  - Preventing Overfitting
    - Using a less expressive model
      - e.g., linear model
    - Adding regularization
      - Promote simpler models
    - Data perturbation (add noise in training)
      - Can be done algorithmically (e.g., dropout)
      - May increase the bias of the model
    - Stop the optimization process earlier
      - Sounds bad in theory, but works in practice

- How Do We Learn?

  - How can we find a good model from the hypothesis space?

- Linear Functions

  - Challenges
    - The hypothesis space contains an infinite number of functions
    - Several functions are consistent with the data
  - A possibility: local search
    - Start with a linear threshold function
    - See how well you're doing
    - Correct
    - Repeat until you converge
    - Optimize a function with calculus

- K-Nearest Neighbor

  - Motivation:

    - Spam
    - Learning from memorization

  - Nearest Neighbors: The Basic Version

    - Training examples are vectors `xi` associated with a label `yi`
      - e.g., `xi` = a feature vector for an email, `yi` = spam
    - Learning: just store all the training examples
    - Prediction: for a new example `x`
      - Find the training example `xi` that is closest to `x`
      - Predict the label of `x` to the label `yi` associated with `xi`

  - K-Nearest Neighbors

    - Training examples are vectors `xi` associated with a label `yi`

      - e.g., `xi` = a feature vector for an email, `yi` = spam

    - Learning: just store all the training examples

    - Prediction: for a new example `x`

      - Find the `k` closest training examples to `x`

    - Construct the label of `x` using these `k` points

    - Issue: how do we define distance?

      - How do we measure distances between instances in vector space?

        - Euclidean distance:

          - $$
            ||x_1-x_2||_2=\sqrt{\sum_{i=1}^n\left(x_{1,i}-x_{2,i}\right)^2}
            $$

        - Manhattan distance:

          - $$
            ||x_1-x_2||_1=\sum_{i=1}^n\left|x_{1,i}-x_{2,i}\right|
            $$

        - `Lp`-norm

          - Euclidean = `L2`, Manhattan = `L1`

          - $$
            ||x_1-x_2||_p=\left(\sum_{i=1}^n\left|x_{1,i}-x_{2,i}\right|^p\right)^{\frac{1}{p}},\quad p>0
            $$

      - In general, a good place to inject knowledge about the domain

      - Behavior of this approach can depend on this

  - Distance Between Instances

    - Most common distance is the Hamming distance
      - Number of bits that are different, or number of features that have a different value



## Lecture 4: K-Nearest Neighbor and the Curse of Dimensionality

- Learning Objectives:

  - KNN algorithms
  - Hyper-parameter tuning
    - Train/develop/test
    - `N`-fold cross-validation
  - Decision boundary
  - Curse of dimensionality
  - Practical concerns – data preprocessing

- KNN Algorithm

  - Training examples are vectors `xi` associated with a label `yi`
  - Learning: just store all the training examples
  - Prediction for a new example `x`:
    - Find the `k` closest training examples to `x`
    - Construct the label of `x` using these `k` points
  - Inductive Bias of KNN
    - Definition of inductive bias: the set of assumptions that the learner uses to predict outputs of unseen results
    - Label of point (data instance) is similar to the label of nearby points
    - Assumption may be broken in some cases, resulting to poor prediction

- Hyper-Parameters & Design Choices

  - Issue in designing KNN algorithm: how do we choose `k` and the distance measure?

  - Hyper-Parameters in KNN

    - Hyper-parameters:

      - Choosing `k` (number of nearest neighbors)

      - Distance measurement (e.g., `p` in the `Lp`-norm)

        - $$
          ||x_1-x_2||_p=\left(\sum_{i=1}^n\left|x_{1,i}-x_{2,i}\right|^p\right)^{\frac{1}{p}}
          $$

    - Those are not measured by the algorithm itself

      - Require empirical studies
      - The best parameter set is task/dataset-specific

  - Train/Dev/Test Splits

    - Split your data into Train/Dev/Test
      - Only use Train and Dev for developing models
    - Train: data for training models
      - Is generally the largest section
    - Dev (aka validation set): find the best parameters by evaluating models on dev
      - Like a practice exam, where Test is the final exam
    - Test: report the performance
      - Labels only revealed in test time
      - Shouldn't be modified often
    - Recipe of Train/Dev/Test
      - For each possible value of the hyper-parameter (e.g., `M = 1, 2, 3, ..., 10`)
        - Train a model using `D^TRAIN`
        - Evaluate the performance on `D^DEV`
      - Choose the model with the best performance on `D^DEV`
      - Optional: re-train the model on `D^TRAIN ∪ D^DEV ` with the best hyper-parameter set
      - Evaluate the final model on `D^TEST`
    - Trade-Off Between Train vs. Dev Size
      - Consider a situation where you have 120 data points and 20 data points are reserved for testing
        - What is the best way to split the remainder?
          - A) Train: 95, Dev: 5?
            - Result on Dev is not representative
          - B) Train: 60, Dev: 40?
            - Not enough data to train a model
    - `N`-Fold Cross Validation
      - Instead of a single Train-Dev split, split data into `N` equal-sized parts
        - Usually `N = 5`
        - Use each part as Dev once, with the rest as Train
      - Train and test `N` different classifiers
      - Report average accuracy and standard deviation of the accuracy
      - Finding Parameters Based on Cross Validation
        - Given `D^TRAIN` and `D^TEST`, for each possible value of the hyper-parameter (e.g., `K = 1, 2, 3, ..., 10`), conduct cross validation on `D^TRAIN` with parameter `K`
        - Choose the model with the best performance on `D^DEV`
        - Optional: re-train the model on `D^TRAIN ∪ D^DEV ` with the best hyper-parameter set
        - Evaluate the final model on `D^TEST`

  - Construct the Label of `x` Using These `k` Points

    - Majority vote
      - To break ties, it's better to use an odd-numbered `k`
    - Weighted vote
      - Weight by their distances; this is related to kernel methods (discussed later)

- Decision Boundary

  - The Decision Boundary for KNN
    - Is the K-nearest neighbors algorithm explicitly building a function?
      - No, it never forms an explicit hypothesis
    - Given a training set, what is the implicit function that is being computed?
    - Ex) If you have two training points, what will the decision boundary for 1-nearest neighbor be?
      - A line bisecting the two points
    - The Voronoi Diagram
      - For any point `x` in a training set `S`, the Voronoi Cell of `x` is a polytype consisting of all points closer to `x` than any other point in `S`
      - The Voronoi Diagram is the union of all Voronoi Cells
        - Covers the entire space

- Curse of Dimensionality

  - Ex) What fraction of the volume of a unit circle lies between radius `1 - ϵ` and radius `1`?

    - $$
      \frac{\pi\times1^2-\pi(1-\epsilon)^2}{\pi\times{1^2}}=1-(1-\epsilon)^2
      $$

  - Ex) What fraction of the volume of a unit sphere lies between radius `1 - ϵ` and radius `1`?

    - $$
      \frac{\frac{4\pi}{3}\times1^2-\frac{4\pi}{3}(1-\epsilon)^2}{\frac{4\pi}{3}\times{1^2}}=1-(1-\epsilon)^3
      $$

  - In `d` dimensions:

    - $$
      1-(1-\epsilon)^d
      $$

      - When `d` is large, this approaches `1`

    - In high dimensions, most of the volume is located far from the center

  - Most of the points in high dimensional space are far away from the origin

    - Need more data to "fill up the space"

  - Bad news for KNN in high-dimensional spaces

    - Even if most/all features are relevant, in high dimensional spaces, most points are equally far away from each other

  - Dealing with the Curse of Dimensionality

    - Most "real-world" data is not uniformly distributed in high dimensional space
      - Capturing the underlying dimensionality of the space – dimensionality reduction

- Data Preprocessing

  - Normalize data to have zero mean and unit standard deviation in each dimension

    - $$
      \bar{x}_d=\frac{1}{N}\sum_nx_{nd},\quad s_d^2=\frac{1}{N-1}\sum_n(x_{nd}-\bar{x}_d)^2
      $$

  - Scale the feature accordingly

    - $$
      x_{nd}\leftarrow\frac{x_{nd}-\bar{x}_d}{s_d}
      $$



## Lecture 5: Decision Trees and Linear Models

- Decision Trees

  - What is a Decision Tree?

    - A hierarchical data structure that represents data
    - Motivation: many decisions are tree structures
    - Terminology:
      - Root: the first point in the tree
      - Internal nodes: nodes in the middle, features used to categorize the data
      - Edge/branch: the different values of features that break the tree into subtrees
      - Leaf: the nodes representing outputs of the model
    - The Representation:
      - Decision trees are classifiers for instances represented as feature vectors
      - Nodes are tests for feature values
      - Edges: there is one branch for each value of the feature
      - Leaves specify the category (labels)
      - Can categorize instances into multiple disjoint categories
    - Handling Real-Valued Features
      - Usually, instances are represented as attribute-value pairs
      - Numerical values can be used by splitting nodes with thresholds
      - A tree partitions the feature space
    - Expressivity of Decision Trees
      - What Boolean functions can a decision tree represent?
        - Any Boolean function

  - Learning a Decision Tree

    - Basic Decision Trees Learning Algorithm

      - Data is processed in batch (i.e., all the data available)
      - Recursively build a decision tree top-down
      - Base case:
        - If all examples are labeled the same, return a single node with the label
      - Otherwise:
        - Pick an attribute and create branches
        - Split the tree

    - Decision Tree Algorithm: ID3

      - ```pseudocode
        ID3(S, Attributes, Label):
        	If all the attributes have the same label:
        		Return a single node with Label // Base case
        	A = attribute in Attributes that best classifies S
        	For each possible value v of A:
        		Add a new tree branch corresponding to A=v
        		Let Sv be the subset of examples in S with A=v
        		If Sv is empty:
        			Add leaf node with the common value of Label in S
          	Else:
          		Add the subtree ID3(Sv, Attributes - {A}, Label)
        ```

    - Which Attribute to Split?

      - The goal is to have the resulting decision tree as small as possible

        - Finding the minimal decision tree consistent with the data is NP-hard
        - We use a greedy heuristic search for a simple tree (cannot guarantee optimality)

      - How do we quantify it?

        - The most popular heuristic is based on the information gain

      - How do we Measure Information Gain?

        - Idea: gaining information reduces uncertainty
        - Uncertainty can be measured by entropy

      - Entropy

        - Entropy (impurity, disorder) of a set of examples, `S`, relative to a binary classification is:

        - $$
          H[S]=-P_+\log_2(P_+)-P_-\log_2(P_-)
          $$

          - Here, we define `0log0 = 0`

        - Where `P+` is the proportion of positive examples in `S` and `P-` is the proportion of negatives

        - Formal Definition:

          - If a random variable `S` has `K` different values, `a1, a2, ..., ak`, its entropy is given by:

            - $$
              H[S]=-\sum_{v=1}^KP(S=a_v)\log_2P(S=a_v)
              $$

      - Information Gain

        - The information gain of an attribute `a` is the expected reduction in entropy caused by partitioning on this attribute

        - $$
          Gain(S,A)=Entropy(S)-\sum_{v\in\text{Values}(A)}\frac{|S_v|}{|S|}Entropy(S_v)
          $$

          - `Sv` is the subset of `S` for which attribute `a` has value `v`
          - The entropy of partitioning the data is calculated by weighing the entropy of each partition by its size

  - Summary:

    - Representation: what are decision trees?
      - A hierarchical data structure that represents data
    - Algorithm: learning decision trees
      - The ID3 algorithm: a greedy heuristic
        - If all the examples have the same label, create a leaf with that label
        - Otherwise, find the "most informative" attribute and split the data for different values of that attribute
        - Recurse on the splits

- Linear Models

  - Recap: `X` as a vector space

    - `X` is an `N`-dimensional vector space (e.g., `R^N`)
      - Each dimension = one feature
    - Each `x` is a feature vector
    - Think of `x = [x1, ..., xN]` as a point in `X`
    - Goal is to find a hyperplane that separates the space

  - Hypothesis Space

    - Design an algorithm to find a good line within the hypothesis space

    - $$
      w^Tx+b=0
      $$

      - `w` and `b` are the parameters to represent a linear function



## Lecture 6: Linear Model and Perceptron

- What We Will Learn Today

  - Linear model

    - Basic linear algebra and linear classifier

    - Trick to remove bias term `b` in:

      - $$
        w^Tx+b=0
        $$

  - Perceptron algorithm

    - Perceptron update
    - Why it works
    - Convergence theorem – mistake bound

- Linear Model

  - Hypothesis Space

    - $$
      w=\begin{bmatrix}
      w_1\\
      w_2\\
      \vdots\\
      \vdots\\
      w_n
      \end{bmatrix},\quad
      x=\begin{bmatrix}
      x_1\\
      x_2\\
      \vdots\\
      \vdots\\
      x_n
      \end{bmatrix}\\
      w^Tx=w_1x_1+w_2x_2+\cdots+w_nx_n
      $$

    - `w^Tx` is the inner product (dot product) between `w` and `x`

    - In `n` dimensions, a linear classifier represents a hyperplane that separates the space into two half spaces

  - Linear Models for Binary Classification

    - Given training set:

      - $$
        D=\{(x,y)\},\quad x\in\mathbb{R}^d,y\in\{-1,+1\}
        $$

      - We use them to learn a hypothesis function `h ∈ H`:

        - $$
          H=\{h\ |\ h(x)=\text{sgn}(w^Tx+b)\}
          $$

          - $$
            \text{sgn}(x)=\begin{cases}
            1&\text{if }z\ge0\\
            -1&\text{otherwise}
            \end{cases}
            $$

          - Note: when `z = 0`, we can either assign `sgn(z) = 1` or `-1`

          - `w` and `n` are model/learnable parameters

        - Such that `y = h(x)`

    - Learn = Train = Find the best parameters `w`, `b`

  - Linear Classifiers

    - Linear classifiers classify an example `x` using the following classification process:

      - Use weighted sum of features (`w`) plus a bias (`b`) and apply the `sgn` function (sign)

    - A Simple Trick to Remove the Bias Term `b`

      - Can also consider an additional feature of weight `b` and value `1` to condense process into a single summation

      - $$
        \tilde{w}=\begin{bmatrix}
        w_1\\
        w_2\\
        \vdots\\
        \vdots\\
        w_n\\
        b
        \end{bmatrix},
        \tilde{x}=\begin{bmatrix}
        x_1\\
        x_2\\
        \vdots\\
        \vdots\\
        x_n\\
        b
        \end{bmatrix}
        $$

  - Learning a Linear Classifier

    - There are several algorithms/models:
      - Perceptron
      - Logistic regression
      - (Linear) support vector machines
      - Naive Bayes
      - etc.
    - Different methods define "best" in a different way

  - Linear Regression

    - Linear regression maps an example `x` into a real value `y`

    - Given training set:

      - $$
        D=\{(x,y)\},\quad x\in\mathbb{R}^d,y\in\{-1,+1\}
        $$

      - We use them to learn a hypothesis function `h ∈ H`:

        - $$
          H=\{h\ |\ h(x)=w^Tx+b\}
          $$

        - No `sgn` function

- Perceptron

  - Mistake + Correction = Learning

  - The Perceptron Algorithm

    - The goal is to find a separating hyperplane

    - An online algorithm

      - Processes one example at a time

    - ```pseudocode
      Initialize w ← 0 ∈ R^n
      For (x, y) in D:
      	y^hat = sgn(w^Tx)						// Predict
        if y&hat ≠ y, w ← w + yx 	// Update
      Return w
      ```

    - Prediction:

      - $$
        y^{test}\leftarrow\text{sgn}(\bold{w}^T\bold{x^{test}})
        $$

  - Intuition Behind Updates

    - Mistake on postive:

      - $$
        \bold{w_{t+1}}\leftarrow \bold{w_t}+\bold{x_i}
        $$

    - Mistake on negative:

      - $$
        \bold{w_{t+1}}\leftarrow \bold{w_t}-\bold{x_i}
        $$

    - Suppose we have made a mistake on a positive example

      - $$
        y=+1,\quad \bold{w_t}^T\bold{x}\le0
        $$

    - Call the new weight vector:

      - $$
        \bold{w_{t+1}}=\bold{w_t}+\bold{x}
        $$

    - The new dot product will be:

      - $$
        \bold{w_{t+1}}^T\bold{x}=(\bold{w_t}+\bold{x})^T\bold{x}=\bold{w_t}^T\bold{x}+\bold{x}^T\bold{}
        $$

    - For a positive example, the Perceptron update will increase the score assigned to the same input

    - Similar reasoning for negative examples

  - Perceptron Learnability

    - Perceptron cannot learn what it cannot represent – only linearly separable functions
      - e.g., parity functions (like XOR) cannot be learned

  - Convergence Theorem

    - If there exists a set of weights that are consistent with the data (i.e., the data is linearly separable), the Perceptron algorithm will converge
      - The update stops after making a finite number of mistakes
      - The convergence rate depends on the difficulty of the problem
    - Note: this is the condition of the data, we may not know what the hyperplane is
    - If the data is not linearly separable, then the algorithm will eventually repeat the same set of weights and enter an infinite loop

  - Margin

    - If a hyperplane can separate the data, the margin of a hyperplane for the dataset is the distance between the hyperplane and the data point closest to it

    - The margin of a dataset (`γ`) is the maximum margin possible for that dataset using any weight vector

    - Let `{(x1, y1), (x2, y2), ..., (xm, ym)}` be a set of training data, if for all data points `(xi, yi)` in the training set, there exists a unit vector `u` such that:

      - $$
        y_i(\bold{u}^T\bold{x_i})\ge\gamma_i
        $$

    - Margin is not scale invariant

      - i.e., if we double the size of every input `xi`, the margin `γ` is also doubled

      - The "difficulty" of the problem can be captured by:

        - $$
          \frac{R}{\gamma},\quad||x_i||\le R,\forall_i
          $$

  - Mistake Bound Theorem

    - Let `{(x1, y1), (x2, y2), ..., (xm, ym)}` be a sequence of training examples such that for all `i`, the feature vector `xi ∈ R^n` `||xi|| ≤ R`, and the label `y ∈ {-1, +1}`

    - Suppose there exists some unit vector `u ∈ R^n` such that for some `γ > 0`, we have:

      - $$
        y_i(\bold{u}^T\bold{x_i})\ge\gamma_i
        $$

    - Then, the number of mistakes the Perceptron algorithm makes on the training set is bounded by:

      - $$
        \left(\frac{R}{\gamma}\right)^2
        $$

  - Beyond the Separable Case

    - Good news: Perceptron makes no assumption about data distribution, could even be adversarial
    - Bad news: Real-world data are often no linearly separable



## Lecture 7: Logistic Regression, Sigmoid Functions, and Gradient Descent

- Recall: The Geometry of a Linear Classifier

  - If we use a unit normal vector `u` to represent the hyperplane, the distance between point `x` to the plane is `|u^Tx + b|` or `y(u^Tx + b)`

    - Hold as long as the model classifies correctly

  - If the distance between the closest point in dataset `D` to the plane `u` is `γ`:

    - $$
      y_i(\bold{u}^Tx_i+b)\ge\gamma,\forall(x_i,y_i)\in D
      $$

    - As long as the dataset is linearly separable

- Logistic Regression

  - What You Will Learn Today

    - Logistic regression assumption
    - Sigmoid function
    - Maximum likelihood principle
    - Optimization in ML
      - Stochastic gradient descent

  - What Makes Data Not Linearly Separable?

    - Decision boundary is nonlinear
      - e.g., XOR
    - Noise in the training data
      - Outlier due to annotation errors
    - Not enough features
    - The nature of the prediction task
      - 

  - Classification, but...

    - The output `y`y is a discrete value
    - Instead of predicting the output label, let's predict `P(y = 1 | x)`
      - How likely a given label is accurate if the feature vector lies in this region
    - Perceptron doesn't produce probability estimates

  - Predict `P(y = 1 | x)`

    - Input:

      - $$
        x\in\mathbb{R}^d
        $$

    - Output:

      - $$
        y\in\{-1,1\}
        $$

    - Build a model `h(x)` such that:

      - $$
        h(\bold{x})=\sigma(\bold{w}^T\bold{x}+b)\approx P(y=1\ |\ \bold{x})
        $$

        - A regression problem

        - `σ` is a sigmoid function:

          - $$
            \begin{equation*}
            \begin{split}
            \sigma(z)&=\frac{\text{exp}(z)}{1+\text{exp}(z)}\\
            & = \frac{1}{1+\text{exp}(-z)}
            \end{split}
            \end{equation*}
            $$

  - How Can We Design Such a Transformation Function?

    - Idea 1: Function always outputs a positive value

      - `exp()` is always positive
      - `exp(w^Tx + b)` always returns a positive value
      - Problem: the function may return a value `>1`

    - Idea 2: Normalize the value such that it is less than `1`

      - `exp(w^Tx + b) ∈ (0, ∞)` grows very fast, so we need to us `exp` to normalize itself

      - Let's use:

        - $$
          \sigma(\bold{w}^T\bold{x}+b)=\frac{\text{exp}(\bold{w}^T\bold{x}+b)}{1+\text{exp}(\bold{w}^T\bold{x}+b)}
          $$

      - $$
        \bold{w}^T\bold{x}+b\rightarrow\infty\Rightarrow\sigma(\bold{w}^T\bold{x}+b)\rightarrow1\\
        \bold{w}^T\bold{x}+b\rightarrow-\infty\Rightarrow\sigma(\bold{w}^T\bold{x}+b)\rightarrow0\\
        $$

  - The Sigmoid Function

    - The `σ(z)` function is called a sigmoid function (or logistic function)

    - $$
      \begin{equation*}
      \begin{split}
      \sigma(z)&=\frac{\text{exp}(z)}{1+\text{exp}(z)}\\
      & = \frac{1}{1+\text{exp}(-z)}
      \end{split}
      \end{equation*}
      $$

  - Summary (Modeling)

    - What is the goal of logistic regression?

      - Model `P(y = 1 |  x)`

    - What is the hypothesis space?

      - $$
        H=\{h\ |\ h:X\rightarrow P(Y\ |\ X),h(\bold{x})=\sigma(\bold{w}^T\bold{x}+b)\}\\
        \sigma(z)=\frac{1}{1+\text{exp(z)}}
        $$

    - We want to find `h(x)` such that:

      - $$
        h(\bold{x})\approx P(y=1\ |\ \bold{x})
        $$

- Decision Boundary of Logistic Regression

  - Predicting a Label

    - $$
      P(y=1\ |\ \bold{x};\bold{w})=\sigma(\bold{w}^T\bold{x})=\frac{1}{1+\text{exp}(-\bold{w}^T\bold{x})}
      $$

    - Compute `σ(w^Tx)`

      - If this is greater than `1/2`, predict `1`, otherwise predict `-1`

- How to Train a Logistic Regression Model

  - Logistic Regression: Setup

    - The Setting

      - Binary classification
      - Inputs: feature vectors `x ∈ R^N`
      - Labels: `y ∈ {-1, +1}`

    - Training Data

      - `S = {(x, y)}`, `m` examples

    - Hypothesis Space

      - $$
        H=\{h\ |\ h:X\rightarrow P(Y\ |\ X),h(\bold{x})=\sigma(\bold{w}^T\bold{x}+b)\}\\
        \sigma(z)=\frac{1}{1+\text{exp(z)}}
        $$

    - Learning Goal

      - Find an `h ∈ H`, such that `h(x) ≈ P(y = 1 | x)`
      - How?

  - Maximum Likelihood

    - Drawing Color Cards from the Envelope

      - Sample with replacement `n` times

      - `k` times we get a purple card and `n - k` times, we get a yellow card

      - The joint probability (likelihood)

        - $$
          C^n_k\theta^k(1-\theta)^{n-k}
          $$

      - What is the best `θ` that maximizes the joint probability?

      - Solving:

        - $$
          \text{max}_\theta\theta^k(1-\theta)^{n-k}
          $$

      - Equivalently, we can solve:

        - $$
          \text{max}_\theta\log(\theta^k(1-\theta)^{n-k})=\text{max}_\theta k\log\theta+(n-k)\log(1-\theta)
          $$

      - At the optimum:

        - $$
          \frac{\text{d}(k\log\theta+(n-k)\log(1-\theta))}{d\theta}=0\\
          \frac{k}{\theta}-\frac{n-k}{1-\theta}=0\\
          \theta(n-k)=(1-\theta)k\\
          \theta=\frac{k}{n}
          $$

    - Maximum Likelihood Estimator (Formal Definition)

      - Likelihood Function of Parameters

        - Let `X1, ..., XN` be IID with PDF `p(x | θ)` => the likelihood function is defined by `L(θ)`:

          - $$
            L(\theta)=p(X_1,\ldots,X_N;\theta)=\prod_{i=1}^Np(X_i;\theta)
            $$

          - Note: the likelihood function is just the joint density of the data, except that we treat it as a function of the parameter `θ`

        - Definition: the maximum likelihood estimator (MLE) `θ^hat` is the value of `θ` that maximizes `L(θ)`

          - The log-likelihood function is defined by `l(θ) = log L(θ)`
          - Its maximum occurs at the same places as that of the likelihood function

      - Maximum Likelihood Estimator for Logistic Regression

        - $$
          \text{argmax}_{w,b}P(S;w,b)=\text{argmax}_{w,b}\prod_{i=1}^mP(y_i\ |\ x_i;w,b)
          $$

        - Equivalent to solve:

          - $$
            \text{argmax}_{w,b}\sum_{i=1}^m\log P(y_i\ |\ x_i;w,b)
            $$

        - Remember our assumption:

          - $$
            P(y=1\ |\ x;w,b)=\sigma(w^Tx+b)=\frac{1}{1+\text{exp}(-(w^Tx+b))}\\
            \begin{equation*}
            \begin{split}
            P(y=-1\ |\ x;w,b)&=1-\sigma(w^Tx+b)\\
            &=1-\frac{1}{1+\text{exp}(-(w^Tx+b))}\\
            &=\frac{\text{exp}(-(w^Tx+b))}{1+\text{exp}(-(w^Tx+b))}\\
            &=\frac{1}{1+\text{exp}(w^Tx+b)}\\
            &=\sigma(-(w^Tx+b))
            \end{split}
            \end{equation*}
            $$

          - $$
            \begin{equation*}
            \begin{split}
            P(y_i\ |\ x_i;w,b)&=\begin{cases}
            \sigma(w^Tx_i+b)\\
            \sigma(-(w^Tx_i+b))
            \end{cases}\\
            &=\sigma(y_i(w^Tx_i+b))
            \end{split}
            \end{equation*}
            $$

          - $$
            \text{argmax}_{w,b}\sum_{i=1}^m\log\sigma(y_i(w^Tx_i+b))=\text{argmax}_{w,b}-\sum_{i=1}^m\log(1+\text{exp}(-y_i(w^Tx_i+b)))
            $$

- How to Optimize the Loss

  - How to Minimize the Loss

    - Optimization methods:
      - Gradient descent
      - Stochastic gradient descent
        - Good enough to give a good model + good performance
      - Analytic solution
      - Many other approaches

  - How to Solve It

    - $$
      \text{argmax}_{w,b}-\sum_{i=1}^m\log(1+\text{exp}(-y_i(w^Tx_i+b)))
      $$

    - There is no closed-form solution

    - Max `f(x)` is equivalent to min `-f(x)`

      - Only need to consider minimization problems

    - One way to solve it is by gradient descent

- Gradient Descent

  - ```pseudocode
    Start at a random point
    Repeat
    	Determine a descent direction
    	Choose a step size
    	Update
    Until stopping criterion is satisfied
    ```

  - Where Will We Converge?

    - If the function is convex, it converges to the global optimum (need proper choice of step size)

    



## Lecture 8: Gradient Descent, Evaluation Metrics, Neural Network, and Deep Learning

- What You Will Learn Today

  - Optimization
    - Gradient descent
    - Stochastic gradient descent (SGD)

  - Evaluation Metrics
  - Neural Network/Deep Learning
    - Non-linear classifier
    - Feed-forward neural network
    - Deep learning architecture

- Gradient Descent

  - Example

    - $$
      \text{min}f(\theta)=0.5(\theta_1^2-\theta_2^2)^2+0.5(\theta_1-1)^2\\
      \nabla f(\theta)=\begin{cases}
      2(\theta_1^2-\theta_2)\theta_1+\theta_1-1\\
      -(\theta_1^2-\theta_2)
      \end{cases}
      $$

    - Use the following iterative procedure for gradient descent:

      - Initialize `θ1` and `θ2` and `t = 0`

      - Do:

        - $$
          \theta_1^{(t+1)}\leftarrow\theta_1^{(t)}-\eta[2(\theta_1^{(t)^2}-\theta_2^{(t)})\theta_1^{(t)}+\theta_1^{(t)}-1]\\
          \theta_2^{(t+1)}\leftarrow\theta_2^{(t)}-\eta[-(\theta_1^{(t)^2}-\theta_2^{(t)})]\\
          t\leftarrow t+1
          $$

      - Until `f(θt)` does not change much

  - Remarks

    - `η` is often called step size or learning rate – how far out update will go along the direction of the negative gradient

    - With a suitable choice of `η`, the iterative procedure converges to a stationary point where:

      - $$
        \frac{\partial f}{\partial\theta}=0
        $$

      - A small `η` is too slow

      - A large `η` is too unstable (may overshoot)

    - A stationary point is only necessary for being the minimum

  - Recap: Logistic Regression

    - Training Data:

      - $$
        S=\{(x_i,y_i)\}
        $$

      - `m` examples

    - Hypothesis Space:

      - $$
        H=\{h\ |\ h:X\rightarrow P(Y\ |\ X),h(x)=\sigma(w^Tx+b)\}
        $$

      - $$
        \sigma(z)=\frac{1}{1+\text{exp}(-z)}
        $$

      - i.e., model `P(Y | X)` by `σ(w^Tx + b)`

    - How to find the best `h ∈ H`: maximum log-likelihood

      - $$
        \text{argmax}-\sum_{i=1}^m\log(1+\text{exp}(-y_i(w^Tx_i+b)))
        $$

  - Gradient Descent for Logistic Regression

    - Maximum log-likelihood:

      - $$
        \text{argmax}-\sum_{i=1}^m\log(1+\text{exp}(-y_i(w^Tx_i+b)))
        $$

    - Equivalent to the following minimization problem:

      - $$
        \text{argmin}\sum_{i=1}^m\log(1+\text{exp}(-y_i(w^Tx_i+b)))
        $$

      - $$
        L(w,b)=\sum_{i=1}^m\log(1+\text{exp}(-y_i(w^Tx_i+b)))
        $$

    - Gradient of `L(w, b)`:

      - $$
        \nabla L(w,b)=\sum^m_{i=1}\nabla\log(1+\text{exp}(-y_i(w^Tx_i+b)))
        $$

  - Recap: Gradient

    - Let `z` be an `n`-dimensional vector of variables, `f(z)` is a function of `z`

    - $$
      \nabla f(z)=\begin{bmatrix}
      \frac{\partial f(z)}{\partial z_1}\\
      \frac{\partial f(z)}{\partial z_2}\\
      \vdots\\
      \frac{\partial f(z)}{\partial z_{n-1}}\\
      \frac{\partial f(z)}{\partial z_n}
      \end{bmatrix}
      $$

    - Exercise

      - Let:

        - $$
          z=\begin{bmatrix}
          z_1,z_2,z_3
          \end{bmatrix}^T,\quad a=\begin{bmatrix}
          3,2,4
          \end{bmatrix}^T
          $$

        - $$
          f(z)=\log(a^Tz)=\log(3z_1+2z_2+4z_3)
          $$

      - $$
        \nabla f(z)=\begin{bmatrix}
        \frac{\partial f(z)}{\partial z_1}\\
        \frac{\partial f(z)}{\partial z_1}\\
        \frac{\partial f(z)}{\partial z_1}\\
        \end{bmatrix}=\begin{bmatrix}
        \frac{3}{3z_1+2z_2+4z_3}\\
        \frac{2}{3z_1+2z_2+4z_3}\\
        \frac{3}{3z_1+2z_2+4z_3}
        \end{bmatrix}=\frac{1}{3z_1+2z_2+4z_3}\begin{bmatrix}
        3\\
        2\\
        4
        \end{bmatrix}=\frac{1}{a^Tz}a
        $$

  - Gradient of `L(w, b)`

    - $$
      \begin{equation*}
      \begin{split}
      \nabla L(w,b)&=\sum^m_{i=1}\nabla\log(1+\text{exp}(-y_i(w^Tx_i+b)))\\
      &=\nabla\log\frac{1}{\sigma(y_i(w^Tx_i+b))}\\
      &=\nabla\log\frac{1}{\sigma(z)}\\
      &=\nabla\log(1+\text{exp}(-z))\\
      &=-\frac{\text{exp}(-z)}{1+\text{exp}(-z)}\\
      &=-\frac{1+\text{exp}(-z)-1}{1+\text{exp}(-z)}\\
      &=-1+\frac{1}{1+\text{exp(-z)}}\\
      &=\sigma(z)-1
      \end{split}
      \end{equation*}
      $$

    - Using the above:

      - $$
        \nabla_wL(w,b)=\sum_{i=1}^m\nabla_w\log\frac{1}{\sigma(y_i(w^Tx_i+b))}=\sum^m_{i=1}(\sigma(y_i(w^Tx_i+b))-1)y_ix_i
        $$

      - $$
        \nabla_bL(w,b)=\sum_{i=1}^m(\sigma(y_i(w^Tx_i+b))-1)y_i
        $$

  - Gradient Descent for Logistic Regression

    - $$
      S=\{(x_i,y_i)\},\quad i=1\ldots m
      $$

    - Initialize `w`:

      - $$
        w\leftarrow0\in\mathbb{R}^n
        $$

    - For epoch `1 ... T`:

      - Compute:

        - $$
          \nabla_wL(w,b)=\sum^m_{i=1}(\sigma(y_i(w^Tx_i+b))-1)y_ix_i\\
          \nabla_bL(w,b)=\sum_{i=1}^m(\sigma(y_i(w^Tx_i+b))-1)y_i
          $$

      - Update `w` and `b`:

        - $$
          w\leftarrow w-\eta\nabla_wL(w,b)\\
          b\leftarrow b-\eta\nabla_bL(w,b)
          $$

    - Return `w` and `b`

  - Remark

    - $$
      \nabla_wL(w,b)=\sum^m_{i=1}(\sigma(y_i(w^Tx_i+b))-1)y_ix_i\\
      \nabla_bL(w,b)=\sum_{i=1}^m(\sigma(y_i(w^Tx_i+b))-1)y_i
      $$

    - For every data point `(xi, yi)`, we need to compute:

      - $$
        (\sigma(y_i(w^Tx_i+b))-1)
        $$

    - Gradient descent usually needs many iterations to converge

    - When size of data (`m`) is large, computing `∇L(w, b)` is expensive

      - Can we skip some data points?

- Stochastic Gradient Descent

  - Incremental/Stochastic Gradient Descent

    - Repeat for each example `(xi, yi)`:
      - Use this example to calculate the approximate gradient and update the model

    - Contrast with batch gradient descent which makes one update to the weight vector for every pass over the data

  - Recap: Gradient Descent

    - $$
      \nabla_wL(w,b)=\sum^m_{i=1}(\sigma(y_i(w^Tx_i+b))-1)y_ix_i
      $$

    - Gradient descent update:

      - $$
        w\leftarrow w-\eta\sum_{i=1}^m\nabla_wL_i(w,b)
        $$

    - Alternative way of gradient update:

      - For `i = 1 ... m`:

        - $$
          w\leftarrow w-\eta\nabla_wL_i(w,b)
          $$

  - Stochastic Gradient Descent

    - $$
      \nabla_wL(w,b)=\sum_{i=1}^m\nabla_wL_i(w,b)=m\frac{\sum_{i=1}^m\nabla L_i(w,b)}{m}=m\text{avg}(\nabla_wL_i(w,b))
      $$

    - $$
      \text{avg}(\nabla_wL_i(w,b))=E_{(x_i,y_i)\sim S}[\nabla_wL_i(w,b)]
      $$

      - Expectation of gradient `Li(w, b)` over dataset `S`

    - Gradient descent update:

      - $$
        w\leftarrow w-\eta\sum_{i=1}^m\nabla_wL_i(w,b)
        $$

    - Stochastic gradient descent:

      - Repeat until converge:

        - Sample a data point `(xi, yi)` from `S`

        - $$
          w\leftarrow w-\eta'\nabla_wL_i(w,b)
          $$

  - Stochastic Gradient Descent for Logistic Regression

    - $$
      S=\{(x_i,y_i)\},\quad i=1\ldots m
      $$

    - Initialize `w`:

      - $$
        w\leftarrow0\in\mathbb{R}^n
        $$

    - For epoch `1 ... T`:

      - Sample a data point `(xi, yi)` from `S`

      - Compute:

        - $$
          \nabla_wL_i(w,b)=\sum^m_{i=1}(\sigma(y_i(w^Tx_i+b))-1)y_ix_i\\
          \nabla_bL_i(w,b)=\sum_{i=1}^m(\sigma(y_i(w^Tx_i+b))-1)y_i
          $$

      - Update `w` and `b`:

        - $$
          w\leftarrow w-\eta\nabla_wL_i(w,b)\\
          b\leftarrow b-\eta\nabla_bL_i(w,b)
          $$

    - Return `w` and `b`

  - The Perceptron Algorithm

    - Given a training set:

      - $$
        D=\{(x,y)\}
        $$

    - Initialize:

      - $$
        w\leftarrow 0\in\mathbb{R}^n
        $$

    - For epoch `1 ... T`:

      - For `(x, y)` in `D`:

        - If `y(w^Tx) < 0`:

          - $$
            w\leftarrow w+\eta yx
            $$

    - Return `w`

    - Prediction:

      - $$
        y^{test}\leftarrow\text{sgn}(w^Tx^{test})
        $$

      - Effectively minimizing:

        - $$
          \sum_i\text{max}(0,1-y_i(w^Tx_i))
          $$

  - Linear Regression

    - Find a line `w^Tx + b` to approximate real-value output `y` based on input `x`
      - e.g., predict house price next year

  - Least Mean Squares (LMS) Regression

    - Given a dataset:

      - $$
        S=\{(x_i,y_i)\}_{i=1\ldots m},\quad x_i\in\mathbb{R}^n,y\in\mathbb{R}
        $$

    - $$
      \text{argmin}_{w,b}\frac{1}{2}\sum_i^m(y_i-(w^Tx_i+b))^2
      $$

    - Learning: minimizing mean squared error

    - Least squares (LSQ): the fitted line is used as a predictor

    - Exercise:

      - Derive the stochastic gradient descent algorithm for solving LMS regression

- Evaluation Metrics

  - Confusion Matrix

    - |                              | True Label Positive | True Label Negative |
      | ---------------------------- | ------------------- | ------------------- |
      | **Predicted Label Positive** | True Positive (TP)  | False Positive (FP) |
      | **Predicted Label Negative** | False Negative (FN) | True Negative (TN)  |

    - $$
      \text{Accuracy}=\frac{TP+TN}{TP+TN+FN+FP}\\
      \text{Precision}=\frac{TP}{TP+FP}\\
      \text{Recall}=\frac{TP}{TP+FN}
      $$

    - F1 Score

      - Harmonic mean of precision and recall

      - $$
        \frac{1}{F_1}=\frac{\left(\frac{1}{P}+\frac{1}{R}\right)}{2}
        $$

      - $$
        F_1=\frac{2TP}{2TP+FP+FN }
        $$

- Neural Networks/Deep Learning

  - Checkpoint: The Bigger Picture

    - Supervised learning: instances, concepts, and hypotheses
    - Specific learners:
      - Decision trees
      - K-NN
      - Perceptron
      - Logistic regression

    - General ML ideas
      - Feature vectors
      - Overfitting
      - Probabilistic model

  - Neural Networks

    - Designed to mimic the brain

    - Artificial neural networks are not nearly as complex or intricate as the actual brain structure

    - Feed-Forward Neural Network

      - Neural networks are made up of nodes or units, connected by links
      - Each link has an associated weight and activation level
      - Each node has an input function (typically summing over weighted inputs), an activation function, and an output

    - Neuron Model Example: Logistic Unit

      - Model takes features as input, multiplies them by some weight, and then passes it through an activation function

    - Activation Functions

      - Sigmoid function

        - $$
          f(x)=\frac{1}{1+e^{-x}}
          $$

      - Hyperbolic tangent

        - $$
          \tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}
          $$

      - Step function

        - $$
          \begin{cases}
          0&\text{if }x<0\\
          1&\text{if }x\ge0
          \end{cases}
          $$

      - Rectified linear unit (ReLU)

        - $$
          (x)^+=\begin{cases}
          0&\text{if }x\le0\\
          x&\text{if }x>0
          \end{cases}
          $$

    - Feed-Forward Process

      - Input layer units are set by some exterior function (think of these as sensors), which causes their output links to be activated at the specified level
      - Working forward through the network, the input function of each unit is applied to compute the input value
      - The activation function transforms this input function into a final values




## Lecture 9: Deep Learning

- What We Will Learn Today

  - Neural Network/Deep Learning
    - Non-linear classifier
    - Feed-forward neural network
    - Back propagation
    - Deep learning architecture

- Neural Network

  - Overview:

    - Multiple layers of linear models

    - For each linear model, we predict the latent state of the inputs to the next layer

    - $$
      a_i^{(j)}=\text{activation of unit }i\text{ in layer }j\\
      \Theta^{(j)}=\text{weight matrix controlling function mapping from layer }j\text{ to layer }j+1
      $$

      - $$
        a_1^{(2)}=g(\Theta_{10}^{(1)}x_0+\Theta_{11}^{(1)}x_1+\Theta_{12}^{(1)}x_2+\Theta_{13}^{(1)}x_3)=g(z_1^{(2)})\\
        a_2^{(2)}=g(\Theta_{20}^{(1)}x_0+\Theta_{21}^{(1)}x_1+\Theta_{22}^{(1)}x_2+\Theta_{23}^{(1)}x_3)=g(z_2^{(2)})\\
        a_3^{(2)}=g(\Theta_{30}^{(1)}x_0+\Theta_{31}^{(1)}x_1+\Theta_{32}^{(1)}x_2+\Theta_{33}^{(1)}x_3)=g(z_3^{(2)})\\
        h_{\Theta}(x)=a_1^{(3)}=g(\Theta_{10}^{(2)}a_0^{(2)}+\Theta_{11}^{(2)}a_1^{(2)}+\Theta_{12}^{(2)}a_2^{(2)}+\Theta_{13}^{(2)}a_3^{(2)})=g(z_1^{(3)})
        $$

    - If network has `sj` units in layer `j` and `sj+1` units in layer `j + 1`, then `Θ^(j)` has dimension `sj-1 × (sj + 1)`

      - $$
        \Theta^{(1)}\in\mathbb{R}^{3\times4},\quad\Theta^{(2)}\in\mathbb{R}^{1\times4}
        $$

  - Feed-Forward Steps

    - $$
      z^{(2)}=\Theta^{(1)}x\\
      a^{(2)}=g(z^{(2)})\\
      \text{Add }a_0^{(2)}=1\\
      z^{(3)}=\Theta^{(2)}a^{(2)}\\
      h_\Theta(x)=a^{(3)}=g(z^{(3)})
      $$

    - Exercise:

      - $$
        \Theta^{(1)}=\begin{bmatrix}
        1&0&0&1\\
        0&-1&1&0\\
        2&0&1&1
        \end{bmatrix},\quad\Theta^{(2)}=\begin{bmatrix}
        0&1&1&0
        \end{bmatrix}\\
        g(z)=\begin{cases}
        0&\text{if }x<0\\
        1&\text{if }x\ge0
        \end{cases}
        $$

      - $$
        \text{What is the output of }x=\begin{bmatrix}
        1\\
        0\\
        2\\
        1
        \end{bmatrix}?
        $$

      - $$
        z^{(2)}=\begin{bmatrix}
        1\\
        2\\
        2\\
        5
        \end{bmatrix}\\
        a^{(2)}=\begin{bmatrix}
        1\\
        1\\
        1\\
        1
        \end{bmatrix}\\
        z^{(3)}=\begin{bmatrix}
        2
        \end{bmatrix}\\
        h_\Theta(x)=\boxed{1}
        $$

      - Why do we need a non-linear activation function? What would happen if `g(z) = z`?

        - $$
          \begin{equation*}
          \begin{split}
          z^{(3)}&=\Theta^{(2)}a^{(2)}\\
          &=\Theta^{(2)}z^{(2)}\\
          &=\Theta^{(2)}\Theta^{(1)}x
          \end{split}
          \end{equation*}
          $$

        - This would just be a linear model

- Non-Linear Representations

  - Simple Example: AND

    - $$
      x_1,x_2\in\{0,1\}\\
      y=x_1\text{ AND }x_2
      $$

    - $$
      h_\Theta(x)=g(-30+20x_1+20x_2)
      $$

  - Example: OR

    - $$
      x_1,x_2\in\{0,1\}\\
      y=x_1\text{ OR }x_2
      $$

    - $$
      h_\Theta(x)=g(-10+20x_1+20x_2)
      $$

    - Can also implement other boolean functions and combinations of boolean functions

  - Example: XNOR

    - AND

      - $$
        h_\Theta(x)=g(-30+20x_1+20x_2)
        $$

    - OR

      - $$
        h_\Theta(x)=g(-10+20x_1+20x_2)
        $$

    - `(NOT x1) AND (NOT x1)`

      - $$
        h_\Theta(x)=g(10-20x_1-20x_2)
        $$

    - Use AND and `(NOT x1) AND (NOT x1)` to generate latent states, which then act as inputs to OR

  - Exercise

    - If all the samples inside the rectangle formed by (0, 0), (0, 2), (5, 2), (5, 0) are positive and all other points are negative, show a feedforward NN can classify all the samples correctly

      - For simplicity, we assume `g(z)` is a step function
      - What are `Θ^(1)` and `Θ(2)`?

    - $$
      \Theta^{(1)}=\begin{bmatrix}
      0&1&0\\
      5&-1&0\\
      0&0&1\\
      2&0&-1
      \end{bmatrix}\\
      \Theta^{(2)}=\begin{bmatrix}
      3.5&1&1&1&1
      \end{bmatrix}
      $$

- Neural Network Learning

  - Maximum Likelihood

    - Training data:

      - $$
        S=\{(x_i,y_i)\}
        $$

      - `m` examples

      - $$
        y_i=\{0,1\}
        $$

    - Consider a NN:

      - $$
        h_\Theta(x)\in[0,1],\text{ modeling }P(y=1\ |\ x)
        $$

        - $$
          a_1^{(2)}=g(\Theta_{10}^{(1)}x_0+\Theta_{11}^{(1)}x_1+\Theta_{12}^{(1)}x_2+\Theta_{13}^{(1)}x_3)=g(z_1^{(2)})\\
          a_2^{(2)}=g(\Theta_{20}^{(1)}x_0+\Theta_{21}^{(1)}x_1+\Theta_{22}^{(1)}x_2+\Theta_{23}^{(1)}x_3)=g(z_2^{(2)})\\
          a_3^{(2)}=g(\Theta_{30}^{(1)}x_0+\Theta_{31}^{(1)}x_1+\Theta_{32}^{(1)}x_2+\Theta_{33}^{(1)}x_3)=g(z_3^{(2)})\\
          h_{\Theta}(x)=a_1^{(3)}=g(\Theta_{10}^{(2)}a_0^{(2)}+\Theta_{11}^{(2)}a_1^{(2)}+\Theta_{12}^{(2)}a_2^{(2)}+\Theta_{13}^{(2)}a_3^{(2)})=g(z_1^{(3)})
          $$

        - Remember in logistic regression:

          - $$
            h_{w,b}(x)=\sigma(w^Tx+b)
            $$

          - If we choose the activation function `g` as a sigmoid function, this part is equivalent to a logistic regression with input `a`:

            - $$
              g(\Theta_{10}^{(2)}a_0^{(2)}+\Theta_{11}^{(2)}a_1^{(2)}+\Theta_{12}^{(2)}a_2^{(2)}+\Theta_{13}^{(2)}a_3^{(2)})
              $$

    - Maximum likelihood estimator:

      - $$
        \Theta^*=\text{argmax}_\Theta L(\Theta;S)\\
        L(\Theta;S)=\prod^m_{i=1}P_\Theta(y_i\ |\ x_i)\\
        P_\Theta(y_i\ |\ x_i)=\begin{cases}
        h_\Theta(x_i),&y_i=1\\
        1-h_\Theta(x_i),&y_i=0
        \end{cases}
        $$

      - We can rewrite `L(Θ;S)` as:

        - $$
          L(\Theta;S)=\prod^m_{i=1}h_\Theta(x_i)^{y_i}(1-h_\Theta(x_i))^{1-y_i}
          $$

    - Minimum negative log-likelihood and cross-entropy loss

      - $$
        \log L(\Theta;S)=\sum_{i=1}^m[y_i\log h_\Theta(x_i)+(1-y_i)\log(1-h_\Theta(x_i))]
        $$

      - Optimal `Θ` can be obtained by solving `argminΘ J(Θ)`:

        - $$
          J(\Theta)=-\sum^m_{i=1}[y_i\log h_\Theta(x_i)+(1-y_i)\log(1-h_\Theta(x_i))]
          $$

        - Equivalent to loss function for logistic regression if:

          - $$
            h_\Theta(x)=\sigma(w^Tx+b)
            $$

      - Stochastic gradient descent

        - ```
          Given a training set D = {(x, y)}
          Initialize Θ <- 0 ∈ R^n
          For epoch 1...T:
          	For (x, y) in D:
          		Update Θ <- Θ - η∇J(Θ)
          Return Θ
          ```

        - Similar to logistic regression

  - Optimizing the Neural Network

    - Need to compute `∇J(Θ)`

    - Chain Rule

      - Given a function:

        - $$
          f(x)=A(B(C(x)))
          $$

      - The derivative is:

        - $$
          f'(x)=A'(B)\times B'(C)\times C'(x)
          $$

- Backpropagation through Computation Graphs

  - We represent the NN as a graph

  - Forward Propagation

    - Pass result of operation along edges to the right

  - Back Propagation

    - Compute `∂s/∂b`:

      - $$
        \frac{\partial s}{\partial b}=\frac{\partial s}{\partial z}\frac{\partial z}{\partial b}=\cdots
        $$

    - We don't need to calculate the gradient, we just focus on adjacent nodes to calculate a partial gradient

    - Single Node:

      - $$
        h=f(z)
        $$

      - `z` => `f` => `h`

      - `∂s/∂z` <= `f` <= `∂s/∂h`

      - $$
        \frac{\partial s}{\partial z}=\frac{\partial s}{\partial h}\frac{\partial h}{\partial z}
        $$

        - `∂s/∂h` is provided by the last layer
        - Only focus on `∂h/∂z`, which we get from `f`

    - Example:

      - $$
        f(x,y,z)=(x+y)\text{max}(y,z)\\
        z=1,y=2,z=0
        $$

      - Draw the computation graph and calculate `∂f/∂x`, `∂f/∂y`, and `∂f/∂z`

        - Forward propagation steps:

          - $$
            a=x+y\\
            b=\text{max}(y,z)\\
            f=ab
            $$

        - $$
          \frac{\partial f}{\partial x}=\frac{\partial f}{\partial a}\frac{\partial a}{\partial x}\\
          \frac{\partial f}{\partial y}=\frac{\partial f}{\partial a}\frac{\partial a}{\partial y}+\frac{\partial f}{\partial b}\frac{\partial b}{\partial y}
          $$

        - Calculate local gradients, plug in numbers, then use above equations to calculate the partial gradients

      - Compute all gradients at once

        - Naïve way to compute gradients: compute each component separately
          - Redundant computation




## Lecture 10: Multiclass Classification

- What You Will Learn Today

  - Deep learning architectures (not on exam)
  - Multiclass Classification
    - One against all
    - One vs. one
    - Multinomial logistic regression
      - Softmax function

- Deep Learning (cont.)

  - Why You Should Understand Backpropagation

    - Modern deep learning libraries implement backpropagation as a black box for you

      - You can take a plane without knowing why it flies

    - Backpropagation doesn't always work perfectly

      - Understanding why is crucial for debugging and improving models

      - Example: Gradient of Sigmoid

        - $$
          \sigma'(x)=\sigma(x)\times(1-\sigma(x))\\
          \lim_{x\rightarrow\infty}(1-\sigma(x))=0,\lim_{x\rightarrow-\infty}\sigma(x)=0
          $$

        - Large or small inputs will just have gradients equal to 0 => requires many iterations for the model to update => range of inputs must be limited

  - More Details

    - Parameter Initialization
      - Normally initialize weights to small random values; various designs
    - Optimizer
      - Usually SGD works
      - Several SGD variants (e.g., ADAM) automatically adjust learning rate based on an accumulated gradient

- Multi-Class Classification

  - Overview

    - Multiclass classification overview
    - Reducing multiclass to binary
      - One-against-all and one vs. one
    - One classifier approach
      - Multiclass logistic regression

  - What is Multiclass?

    - $$
      \text{Output}\in\{1,2,3,\ldots,K\}
      $$

      - In some cases, output space can be very large (i.e., `K` is very large)

    - Each input belongs to exactly one class (c.f., in multilabel, input belongs to many classes)

  - Two Key Ideas to Solve Multiclass

    - Reducing multiclass to binary
      - Decompose the multiclass prediction into multiple binary decisions
      - Make the final decision based on these binary classifiers
    - Training a single classifier
      - Consider all cases simulataneously

  - One Against All Learning

    - Multiclass classifier

      - $$
        f:\mathbb{R}^n\rightarrow\{1,2,3,\ldots,K\}
        $$

    - Decompose into binary problems

    - "Does the new point belong to this class?"

    - Ideal case: only the correct label will have a positive score

    - Algorithm:

      - Learning:

        - Given a dataset:

          - $$
            D=\{(x_i,y_i)\}
            $$

        - $$
          x_i\in\mathbb{R}^n,\quad y_i=\{1,2,3,\ldots,K\}
          $$

      - Decompose into `K` binary classification tasks

        - Learn `K` models:

          - $$
            w_1,w_2,w_3,\ldots,w_K
            $$

        - For class `k`, construct a binary classification task as:

          - Positive examples: elements of `D` with label `k`
          - Negative examples: all other elements of `D`

        - The binary classification can be solved by any algorithm we've seen

      - Inference: "winner takes all"

        - $$
          \hat{y}=\text{argmax}_{y\in\{1,2,\ldots,K\}}w_y^Tx
          $$

        - For example:

          - $$
            y=\text{argmax}(w_{black}^Tx,w_{blue}^Tx,w_{green}^Tx)
            $$

        - An instance of the general form:

          - $$
            \hat{y}=\text{argmax}_{y\in Y}f(y;w,x)
            $$

          - $$
            w=\{w_1,w_2,\ldots,w_K\},\quad f(y;w,x)=w_y^Tx
            $$

    - Analysis:

      - Not always possible to learn
        - Assumption: each class is individually separable from all the others
        - Need to make sure the range of all classifiers is the same
          - `K` classifiers are trained independently 
        - Easy to implement, works well in practice

  - One vs. One Learning

    - Multiclass classifier

      - $$
        f:\mathbb{R}^n\rightarrow\{1,2,3,\ldots,K\}
        $$

    - Decompose into binary problems

    - "Does the data point belong to this class or that class?"

    - Algorithm:

      - Learning:

        - Given a dataset:

          - $$
            D=\{(x_i,y_i)\}
            $$

        - $$
          x_i\in\mathbb{R}^n,\quad y_i=\{1,2,3,\ldots,K\}
          $$

      - Decompose into `C(K, 2)` binary classification tasks

        - Learn `C(K, 2)` models:

          - $$
            w_1,w_2,w_3,\ldots,w_{K\times\frac{K-1}{2}}
            $$

        - For each class pair `(i, j)`, construct a binary classification task as:

          - Positive examples: elements of `D` with label `i`
          - Negative examples: elements of `D` with label `j`
          - The binary classification can be solved by any algorithm we've seen

      - Inference:

        - Decision options:
          - More complex; each label gets `k - 1` votes
          - Output of the binary classifier may not be coherent
          - Majority: classify example `x` to take label `i` if `i` wins on `x` more often than `j` (`j = 1, ..., k`)
            - If there is a tie, you can look at specific classifications to break it

  - Comparisons

    - One Against All
      - `O(K)` weight vectors to train and store
      - Training set of the binary classifiers may be unbalanced
      - Less expressive; makes a strong assumption
    - One vs. One (All vs. All)
      - `O(K^2)` weight vectors to train and store
      - Size of training set for a pair of labels could be small => overfitting of the binary classifiers
      - Need large space to store model

  - Exercise:

    - Consider we have a 10-class classification problem with 29 features, and each class has 1000 examples

    - How many parameters are in total for linear models with one vs. one?

      - $$
        {10\choose2}\times(29+1)=1305
        $$

    - How many parameters are in total for linear models with one against all?

      - $$
        10\times(29+1)=300
        $$

    - How large is the training data for each one vs. one classifier?

      - $$
        1000+1000=2000
        $$

    - How large is the training data for each one against all classifier?

      - $$
        10\times1000=10000
        $$

  - Problems with Decompositions

    - Learning optimizes over local metrics
      - Does not guarantee good global performance
      - We don't care about the performance of the local classifiers
    - Poor decomposition => poor performance
      - Difficult local problems
      - Irrelevant local problems
    - Efficiency (e.g., All vs. All vs. One vs. All)

- Multiclass Logistic Regression

  - Recall: Binary Logistic Regression

    - $$
      \text{min}_w\frac{1}{2}w^Tw+C\sum_i\log(1+e^{-y_i(w^Tx_i)})
      $$

    - Assume labels are generated using the following probability distribution:

      - $$
        P(y=1\ |\ x,w)=\frac{e^{w^Tx}}{1+e^{w^Tx}}=\frac{1}{1+e^{-w^Tx}}\\
        P(y=-1\ |\ x,w)=\frac{1}{1+e^{w^Tx}}
        $$

  - Multiclass Log-Linear Model

    - Assumption:

      - $$
        P(y\ |\ x,w)=\frac{\text{exp}(w_y^Tx)}{\sum_{y'\in\{1,2,\ldots, K\}}\text{exp}(w_{y'}^Tx)}
        $$

        - The denominator is called the partition function and is used to normalize the probability value
        - Soft-max function

      - This is a valid probability assumption, why?

  - Why do We Call it Softmax?

    - Softmax: let `s(y)` be the score for output `y`

      - Here:

        - $$
          s(y)=w^T\phi(x,y)
          $$

      - $$
        P(y)=\frac{\text{exp}(s(y))}{\sum_{y'\in\{1,2,\ldots,K\}}\text{exp}(s(y))}
        $$

    - We can control the "peakedness" of the distribution using `σ`

      - $$
        P(y\ |\ \sigma)=\frac{\text{exp}(s(y)/\sigma)}{\sum_{y'\in\{1,2,\ldots,K\}}\text{exp}(s(y)/\sigma)}
        $$

    - Softer version of the max function

  - Maximum Log-Likelihood Estimation

    - Training can be done by maximum log-likelihood estimation

      - $$
        \text{max}_w\log P(D\ |\ w)
        $$

    - $$
      D=\{(x_i,y_i)\}\\
      P(D\ |\ w)=\prod_i\frac{\text{exp}(w_{y_i}^Tx_i)}{\sum_{y'\in\{1,2,\ldots,K\}}\text{exp}(w_{y'}^Tx_i)}\\
      \log P(D\ |\ w)=\sum_i[w_{y_i}^Tx_i-\log\sum_{y'\in\{1,2,\ldots,K\}}\text{exp}(w_{y'}^Tx_i)]
      $$

    - Comparisons:

      - Log-linear model (multiclass):

        - $$
          \text{min}_w\sum_i[\log\sum_{k\in\{1,2,\ldots,K\}}\text{exp}(w_k^Tx_i)=w_{y_i}^Tx_i]
          $$

      - Log-linear model (logistic regression)

        - $$
          \text{min}_w\sum_i\log(1+e^{-y_i(w^Tx_i)})
          $$

  - Reduction vs. Single Classifier

    - Reduction
      - Future-proof: if we improve the binary classification model => improve multi-class classifier
      - Easy to implement
    - Single Classifier
      - Global optimization: directly minimize the empirical loss; easier for joint prediction



## Lecture 11: Computational Learning Theory

- This Lecture

  - The Theory of Generalization
    - Difference between learning and memorizing
  - Probably Approximately Correct (PAC) Learning
    - How many training samples do you need to get a good classifier?
      - Good classifier = with high probability, the error is low
    - We will use the monotone conjunction function class as an example

- Computational Learning Theory

  - When can we say a concept is learnable?
    - Learning vs. memorization
    - Don't need to see all samples to make a good prediction
    - Ex) Can you compute `1234 + 2332`? => easy, but you haven't seen this specific example before
  - How much training data do we need to train a good classifier?
    - Sample complexity
  - PAC learnable => if the number of examples we need to see is polynomial to the parameters defining the concept (details will be discussed later)

- Learning Monotone Conjunctions

  - Hypothesis class:

    - $$
      f=x_1,\quad f=x_2,\quad f=x_1\land x_2,\quad f=x_1\land x_2\land x_3
      $$

  - Target function in hindsight:

    - $$
      f=x_2\land x_3
      $$

  - Exercise:

    - Hypothesis class: monotone conjunctions

    - Given the following data:

      - $$
        <(1,1,1),1>\\
        <(1,0,1),0>\\
        <(0,1,1),1>\\
        <(1,1,0),0>
        $$

    - Predict `<(0, 1, 0), ?>`

      - $$
        f=x_2\land x_3\\
        <(0,1,0),0>
        $$

  - Supervised Learning

    - Teacher provides a set of examples `(x, f(x))`

    - The setup:

      - Instance space: `X`, the set of examples

      - Concept space: `C`, the set of possible target functions:

        - The hidden target function is:

          - $$
            f\in C
            $$

        - e.g., all `n`-conjunctions, all `n`-dimensional linear functions, etc.

      - Hypothesis space: `H`, the set of possible hypotheses

        - This is the set that the learning algorithm explores
        - Concept Space vs. Hypothesis Space
          - Concept space = hypothesis space
            - We will work in this setting in this lecture
          - Concept space ⊂ hypothesis space
          - Concept space ⊄ hypothesis space

      - Training instances: positive and negative examples of the target concept drawn from distribution `D`

        - $$
          S\subseteq D
          $$

        - $$
          <x_1,f(x_1)>,<x_2,f(x_2)>,\ldots,<x_n,f(x_n)>
          $$

      - What we want: a hypothesis `h ∈ H` such that `h(x) = f(x)`

      - Assumption: training and test data are both drawn IID from `X`

  - Simple Algorithm

    - Start with having all literals in the monotone conjunction and remove literal `j` if `xj = 0` in some positive instances

    - Runs into problems, as too many literals can be included if the training data is missing cases

      - We argue that if there is a sufficient amount of training data, the probability of such an error occurring is very low

    - Not possible to eliminate a literal that is in the target function

    - Question: how likely is it to incorrectly include a literal when we've already seen `N` examples?

      - Intuitively, with more training data, it's unlikely we never see such a literal in positive training examples, but see it during testing if data is sampled from the same distribution

      - "The Future Will Be Like the Past"

        - We have seen many examples (drawn according to the distribution `D`)

        - Since in all the positive examples `xi` was active, it is very *likely* that it will be active in future positive examples

        - Otherwise, `xi` is active only in a small percentage of the examples, so our error will be small

        - Illustrative Example:

          - Scenario 1: 10 red balls with 20 blue balls

            - How likely is it that we never see a red ball in 100 training instances, but do in a test instance?

              - $$
                P_{train}=\left(\frac{2}{3}\right)^{100}\approx2.45\times10^{-18}\\
                P_{test}=\frac{1}{3}\\
                P_{train}\times P_{test}\approx0
                $$

            - When the number of training points is large, it's unlikely we never see a red ball

          - Scenario 2: 1 red ball with 29 blue balls

            - How likely is it that we never see a red ball in 100 training instances, but do in a test instance?

              - $$
                P_{train}=\left(\frac{29}{30}\right)^{100}=0.037\\
                P_{test}=\frac{1}{30}\\
                P_{train}\times P_{test}=0.0011
                $$

            - If there is very few red balls, we may miss them in training, but the probability we see them in testing is very low

      - Error of a Hypothesis

        - Definition: given a distribution `D` over examples, the error of a hypothesis `h` with respect to a target concept `f` is:

          - $$
            \text{err}_D(h)=\text{Pr}_{x\sim d}[h(x)\ne f(x)]
            $$

- PAC Learning

  - A framework for batch learning

    - Train on a fixed training set
    - Then deploy it in the wild

  - How well will your learning algorithm do in *future* instances?

  - We will first analyze an algorithm for learning conjunctions, then we will define PAC learning

  - Intuition of PAC Learning

    - With the IID sampling assumption, if a concept is reasonable, after we've seen enough examples, it's unlikely to have many error points
    - With the IID sampling assumption, if a concept is too complicated, we need to see an exponential number of samples in order to rule out error points

  - PAC Learning for Monotone Conjunctions

    - Consider the concept space and hypothesis space are both monotone conjunctions with `n` variables

    - Algorithm: start with having all literals in the monotone conjunction and remove literal `j` if `xj = 0` in some positive instances

    - With probability `1 - δ`, the above algorithm requires ??? examples to achieve an error rate less than `ε`

      - e.g., how many examples do we need to ensure the error <5% with 99% probability (`δ = 1%`, `ε = 5%`)

    - Let's consider the case `n = 10`, `δ = 1%`, `ε = 5%`, how likely is the learned `h` wrong?

      - When will we make a mistake?

        - `h` includes some "bad literal" `z`, where we never see `(xz = 0, y = 1)` in training, but see it in test time
        - Let `P(z)` be the probability that `z` is a bad literal

      - To achieve 5% error rate, it's sufficient to ensure the probability of each bad literal:

        - $$
          P(z)<\frac{\epsilon}{n}=0.5\%
          $$

      - There are two cases:

        - The probability of seeing `(xz = 0, y = 1)` < 0.5%
          - The probability of seeing it in test time is already < 0.5%
        - The probability of seeing `(xz = 0, y = 1)` > 0.5%
          - We can bound the number of examples we need to ensure `P(z) < 0.5%`

      - Analysis:

        - We assume the probability of seeing `(xz = 0, y = 1) > ε/n`

          - $$
            \text{Pr}(\text{A bad literal is not eliminated by an example})<1-\frac{\epsilon}{n}
            $$

        - But say we have `m` training examples, then:

          - $$
            \text{Pr}(\text{A bad literal is not eliminated by an example})<\left(1-\frac{\epsilon}{n}\right)^m
            $$

        - There are at most `n` bad literals, so:

          - $$
            \text{Pr}(\text{Any bad literal survives }m\text{ examples})<n\left(1-\frac{\epsilon}{n}\right)^m
            $$

        - We want this probability to be smaller than `δ`

          - Why? So we can choose enough training examples so that the probability that any `z` survives all of them is less than `δ`

          - $$
            n\left(1-\frac{\epsilon}{n}\right)^m<\delta
            $$

        - We know that `1 - x < e^-x`, so it's sufficient to require:

          - $$
            ne^{-\frac{m\epsilon}{n}}<\delta\\
            m>\frac{n}{\epsilon}\left(\ln(n)+\ln\left(\frac{1}{\delta}\right)\right)
            $$

    - Theorem: if we are learning a monotone conjunctive concept with `n`-dimensional Boolean features using `m` training examples, with probability > `1 - δ`, the error of the learned hypothesis `errD(h)` will be less than `ε` if:

      - $$
        m>\frac{n}{\epsilon}\left(\ln(n)+\ln\left(\frac{1}{\delta}\right)\right)
        $$

        

  



## Lecture 12:
