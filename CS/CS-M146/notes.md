# COM SCI M146 - Fall '22 - Chang

[TOC]

## Lecture 1: Introduction

- What is Machine Learning?
  - Machine learning is the study of algorithms that improve on task `T` with respect to performance `P`, based on experience `E`
    - Example:
      - `T`: Recognizing handwritten words
      - `P`: Percentage of words correctly classified
      - `E`: Database of human-labeled images of handwritten words
  - A well-defined learning task is given by `<P, T, E>`
- Prerequisites
  - The pillars of machine learning
    - Probability and statistics
    - Linear algebra
    - Calculus/optimization
  - Computer science background
    - Algorithms
    - Programming experience
      - We will use Python and `scikit-learn`
- Goals of this Course
  - Fundamental concepts and algorithms
    - Customize your own algorithm
  - Common techniques/tools used
    - Theoretical understanding
    - Practical implementation
    - Best practices
  - How to "debug" an ML system
  - Black magic => systematic process
  - What will we learn?
    - Supervised learning
      - Decision tree, Perceptron, linear models, support vector machines, kernel methods, probabilistic models
    - Unsupervised learning
      - Clustering, EM algorithms
    - Learning theory
    - Deep learning (representation learning)
    - Practical issues
      - Experimental evaluation, implementing ML models
- Current Status
  - Compelling results on benchmarks
  - Works well in the general domain
  - Commercial uses
  - Challenges:
    - Incorporate with human knowledge
    - Reliable (fair, robust, interpretable) models that earn human trust
    - Applications in specific domains
    - Skewness of available annotation data
- Machine Learning is Interdisciplinary
  - Makes use of probability and statistics, linear algebra, calculus, theory of computation
  - Related to philosophy, psychology, neurobiology, linguistics, vision, robotics, etc.
  - Has applications in AI (natural language, vision, planning, HCI), engineering (agriculture, civil, etc.), and computer science (compilers, architecture, systems, databases, etc.)



## Lecture 2: Learning and Challenges in ML

- Learning Protocols

  - In general, we have an item `x` drawn from an input space `X` and an item `y` drawn from an output space `Y`
    - We're considering systems that apply a function `f()` to input items `x` and returns an output `y = f(x)`
  - Note that the boundary between supervised learning and unsupervised learning is blurry
  - Supervised Learning
    - Collects labeled data during the training phase
      - Ex) Pictures that are lions and pictures that aren't lions 
    - Performs the correct task during the test phase
    - In supervised learning, we have an item `x` drawn from an instance space `X` and an item `y` drawn from an label space `Y`, connected by a learned model that approximates this system, `y = g(x)`
    - A learning algorithm can be applied to the raw test data, generating a learned model `g(x)`
    - Some labeled data is reserved for testing, where test labels, `Y`, can be compared to the predicted labels, `g(X)`, generated by the learned model for evaluation
      - Analogous to a graded exam, where the student's answers are the predicted labels and the professor's answers are the test labels
  - Unsupervised Learning
    - Given: unlabeled, raw inputs
    - Goal: learn some intrinsic structure in inputs
      - Seems unpromising, but humans do it all the time
      - Ex) Clustering of minions/monsters, deciphering "nice to meet you"
  - Reinforcement Learning
    - Given sequences of states and actions with rewards
    - Learn policy that maximizes agent's reward
    - Analogous to training an animal with positive reinforcement

- Challenges in ML

  - Structured Inference
    - Sometimes, you need context to process data correctly
  - Robustness
    - Machines may struggle with some distinctions that humans can understand
    - Ex) Car vs. shoe
  - Adversarial Attack
    - Small perturbations may cause machines to label incorrectly, even if they make no difference to humans
    - Ex) Self-driving cars identifying speed limits
  - Common Sense
    - Humans have common sense, which allows conclusions to be drawn, even when information is missing
    - Machines have no such common sense
      - Winograd Schema (1972)
        - "The city councilmen refused the demonstrators a permit because they feared violence" vs. "The city councilmen refused the demonstrators a permit because they advocated violence"
        - Who is "they" referring to? => requires reasoning through common sense
      - Visual common sense
        - Ex) It's raining in an image since people are holding umbrellas => machines will search for the raindrops themselves and can't make the connection between umbrellas and rain
  - Fairness/Inclusion in ML
    - If a system works well 99% of the time, but fails for a specific group of people, it can cause problems
    - Complete representation is needed during the machine's training
    - Word Embedding Bias
      - Vectors can be used to associate words to genders
        - Ex) Man => uncle while woman => aunt
      - Runs into problems by capturing stereotypes
        - Ex) Man => doctor while woman => nurse
      - Language generations can be gendered
        - Ex) Misgendering in NLG

- Framing a Learning Problem

  - The Badges Game

    - An example of supervised learning
    - Conference attendees to the 1994 ML conference were given name badges labeled with + or -
    - What function was used to assign these labels?
      - If we saw another set of people, we should know how to assign +s and -s
      - If the second letter was a vowel => +, otherwise => -
      - In general, pick the simpler rule if multiple are possible

  - Using Supervised Learning

    - What is our instance space?

      - What kind of features are we using?
      - `x` is represented in a feature space
        - Typically `x ∈ {0, 1}^n` or `x ∈ R^n`
          - Boolean or real number
        - Usually represented as a vector, called an input vector
          - `X` is an `N`-dimensional vector space (e.g., `R^N`)
            - Each dimension = one feature
          - Each `x` is a feature vector
          - Think of `x = [x1, ..., xN]` as a point in `X`
        - Features can be human-defined, and the machine is tasked with searching through the feature space to find relevant ones
      - Ex) Badges game
        - Possible features: length of first/last name, contains letter "x", number of vowels in name, etc.
      - Good features are essential
        - The choice of features is crucial for how well a task can be learned
          - In many application areas (language, vision, etc.), a lot of work goes into designing suitable features
          - This requires domain expertise
        - This class won't cover what specific features to use for your task, but will touch on some general principles

    - What is our label space?

      - What kind of learning task are we dealing with?
      - `y` is represented in output/label space
      - Different kinds of output:
        - Binary classification: `y ∈ {-1, 1}`
          - Ex) Is this a lion?
        - Multiclass classification: `y ∈ {1, 2, 3, ..., K}`
          - Can be reduced into a binary classification
          - Ex) Is this a lion, cat, or dog?
        - Regression: `y ∈ R`
        - Structured output: `y ∈ {1, 2, 3, ..., K}^N`
          - Not covered in this course
          - A multilabel output (type of structured output) may have multiple answers
            - Ex) Is this a lion, cat, dog, or mammal?
      - In the context of this course, we will assume the features/output type are known

    - What is our hypothesis space?

      - What kind of functions (models) are we learning?

      - The machine can try many functions quickly, but doesn't have the intuition that we do to guide that search

      - We need to choose what kind of model we want to learn

      - A function `g` is consistent to a dataset:

        - $$
          D=\{(x_i,y_i)\}\text{ if }g(x_i)=y_i,\forall i
          $$

      - How many possible functions exist over 4 binary inputs?

        - Complete ignorance: there are `2^16` possible functions

    - What learning algorithm do we use?

      - How do we learn the model from the labeled data?

    - What is our loss function/evaluation metric?

      - How do we measure success? What drives learning?
      - Can be used by the model as a signal to improve



## Lecture 3:

- 