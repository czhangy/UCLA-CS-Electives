# COM SCI M146 - Fall '22 - Chang

[TOC]

## Lecture 1: Introduction

- What is Machine Learning?
  - Machine learning is the study of algorithms that improve on task `T` with respect to performance `P`, based on experience `E`
    - Example:
      - `T`: Recognizing handwritten words
      - `P`: Percentage of words correctly classified
      - `E`: Database of human-labeled images of handwritten words
  - A well-defined learning task is given by `<P, T, E>`
- Prerequisites
  - The pillars of machine learning
    - Probability and statistics
    - Linear algebra
    - Calculus/optimization
  - Computer science background
    - Algorithms
    - Programming experience
      - We will use Python and `scikit-learn`
- Goals of this Course
  - Fundamental concepts and algorithms
    - Customize your own algorithm
  - Common techniques/tools used
    - Theoretical understanding
    - Practical implementation
    - Best practices
  - How to "debug" an ML system
  - Black magic => systematic process
  - What will we learn?
    - Supervised learning
      - Decision tree, Perceptron, linear models, support vector machines, kernel methods, probabilistic models
    - Unsupervised learning
      - Clustering, EM algorithms
    - Learning theory
    - Deep learning (representation learning)
    - Practical issues
      - Experimental evaluation, implementing ML models
- Current Status
  - Compelling results on benchmarks
  - Works well in the general domain
  - Commercial uses
  - Challenges:
    - Incorporate with human knowledge
    - Reliable (fair, robust, interpretable) models that earn human trust
    - Applications in specific domains
    - Skewness of available annotation data
- Machine Learning is Interdisciplinary
  - Makes use of probability and statistics, linear algebra, calculus, theory of computation
  - Related to philosophy, psychology, neurobiology, linguistics, vision, robotics, etc.
  - Has applications in AI (natural language, vision, planning, HCI), engineering (agriculture, civil, etc.), and computer science (compilers, architecture, systems, databases, etc.)



## Lecture 2: Learning and Challenges in ML

- Learning Protocols

  - In general, we have an item `x` drawn from an input space `X` and an item `y` drawn from an output space `Y`
    - We're considering systems that apply a function `f()` to input items `x` and returns an output `y = f(x)`
  - Note that the boundary between supervised learning and unsupervised learning is blurry
  - Supervised Learning
    - Collects labeled data during the training phase
      - Ex) Pictures that are lions and pictures that aren't lions 
    - Performs the correct task during the test phase
    - In supervised learning, we have an item `x` drawn from an instance space `X` and an item `y` drawn from an label space `Y`, connected by a learned model that approximates this system, `y = g(x)`
    - A learning algorithm can be applied to the raw test data, generating a learned model `g(x)`
    - Some labeled data is reserved for testing, where test labels, `Y`, can be compared to the predicted labels, `g(X)`, generated by the learned model for evaluation
      - Analogous to a graded exam, where the student's answers are the predicted labels and the professor's answers are the test labels
  - Unsupervised Learning
    - Given: unlabeled, raw inputs
    - Goal: learn some intrinsic structure in inputs
      - Seems unpromising, but humans do it all the time
      - Ex) Clustering of minions/monsters, deciphering "nice to meet you"
  - Reinforcement Learning
    - Given sequences of states and actions with rewards
    - Learn policy that maximizes agent's reward
    - Analogous to training an animal with positive reinforcement

- Challenges in ML

  - Structured Inference
    - Sometimes, you need context to process data correctly
  - Robustness
    - Machines may struggle with some distinctions that humans can understand
    - Ex) Car vs. shoe
  - Adversarial Attack
    - Small perturbations may cause machines to label incorrectly, even if they make no difference to humans
    - Ex) Self-driving cars identifying speed limits
  - Common Sense
    - Humans have common sense, which allows conclusions to be drawn, even when information is missing
    - Machines have no such common sense
      - Winograd Schema (1972)
        - "The city councilmen refused the demonstrators a permit because they feared violence" vs. "The city councilmen refused the demonstrators a permit because they advocated violence"
        - Who is "they" referring to? => requires reasoning through common sense
      - Visual common sense
        - Ex) It's raining in an image since people are holding umbrellas => machines will search for the raindrops themselves and can't make the connection between umbrellas and rain
  - Fairness/Inclusion in ML
    - If a system works well 99% of the time, but fails for a specific group of people, it can cause problems
    - Complete representation is needed during the machine's training
    - Word Embedding Bias
      - Vectors can be used to associate words to genders
        - Ex) Man => uncle while woman => aunt
      - Runs into problems by capturing stereotypes
        - Ex) Man => doctor while woman => nurse
      - Language generations can be gendered
        - Ex) Misgendering in NLG

- Framing a Learning Problem

  - The Badges Game

    - An example of supervised learning
    - Conference attendees to the 1994 ML conference were given name badges labeled with + or -
    - What function was used to assign these labels?
      - If we saw another set of people, we should know how to assign +s and -s
      - If the second letter was a vowel => +, otherwise => -
      - In general, pick the simpler rule if multiple are possible

  - Using Supervised Learning

    - What is our instance space?

      - What kind of features are we using?
      - `x` is represented in a feature space
        - Typically `x ∈ {0, 1}^n` or `x ∈ R^n`
          - Boolean or real number
        - Usually represented as a vector, called an input vector
          - `X` is an `N`-dimensional vector space (e.g., `R^N`)
            - Each dimension = one feature
          - Each `x` is a feature vector
          - Think of `x = [x1, ..., xN]` as a point in `X`
        - Features can be human-defined, and the machine is tasked with searching through the feature space to find relevant ones
      - Ex) Badges game
        - Possible features: length of first/last name, contains letter "x", number of vowels in name, etc.
      - Good features are essential
        - The choice of features is crucial for how well a task can be learned
          - In many application areas (language, vision, etc.), a lot of work goes into designing suitable features
          - This requires domain expertise
        - This class won't cover what specific features to use for your task, but will touch on some general principles

    - What is our label space?

      - What kind of learning task are we dealing with?
      - `y` is represented in output/label space
      - Different kinds of output:
        - Binary classification: `y ∈ {-1, 1}`
          - Ex) Is this a lion?
        - Multiclass classification: `y ∈ {1, 2, 3, ..., K}`
          - Can be reduced into a binary classification
          - Ex) Is this a lion, cat, or dog?
        - Regression: `y ∈ R`
        - Structured output: `y ∈ {1, 2, 3, ..., K}^N`
          - Not covered in this course
          - A multilabel output (type of structured output) may have multiple answers
            - Ex) Is this a lion, cat, dog, or mammal?
      - In the context of this course, we will assume the features/output type are known

    - What is our hypothesis space?

      - What kind of functions (models) are we learning?

      - The machine can try many functions quickly, but doesn't have the intuition that we do to guide that search

      - We need to choose what *kind* of model we want to learn

      - A function `g` is consistent to a dataset:

        - $$
          D=\{(x_i,y_i)\}\text{ if }g(x_i)=y_i,\forall i
          $$

      - How many possible functions exist over 4 binary inputs?

        - Complete ignorance: there are `2^16` possible functions

    - What learning algorithm do we use?

      - How do we learn the model from the labeled data?

    - What is our loss function/evaluation metric?

      - How do we measure success? What drives learning?
      - Can be used by the model as a signal to improve



## Lecture 3: Hypothesis Space & KNN

- Using Supervised Learning (cont.)

  - What is our hypothesis space?

    - How many possible functions are consistent with the training data? (7 known inputs, 4 inputs)

      - `2^9` possible functions
      - Is learning possible?

    - The number of possible functions `f(x)` from the instance space `X` to the label space `Y` is:

      - $$
        |Y|^{|X|}
        $$

    - Learners typically consider only a subset of the functions from `X` to `Y`, called the hypothesis space `H`:

      - $$
        H\subseteq|Y|^{|X|}
        $$

    - Simple rules: conjunctive rules of the form:

      - $$
        y=x_i\land x_j\land\ ...\land\ x_k
        $$

    - `m`-of-`n` rules: if and only if at least `m` of the following `n` variables are `1`

- Views of Learning

  - Learning is the removal of our remaining uncertainty	
    - Ruling out of inconsistent functions in the hypothesis space using training data
  - Learning requires guessing a good hypothesis class
    - If it's too restrictive, you may find no consistent functions; if it's too general, you may find many consistent functions
    - Start with a small class and enlarge it until it contains a hypothesis that fits the data
  - We could be wrong!
    - Our guess of the hypothesis space could be wrong
    - It may even be possible that the hypothesis is consistent with the training data, but we're still off

- General Strategies for Machine Learning

  - Develop flexible hypothesis spaces
    - Decision trees, neural networks, nested collections, etc.
  - Develop algorithms for finding the "best" hypothesis in the hypothesis space that fits the data
    - Also, hope that it will generalize well

- Hypothesis Space – Real Value Features

  - Strong assumption that the data given in the training set will follow a given pattern => leveraged to create the hypothesis set
  - Underfitting and Overfitting
    - Goal is to find a hypothesis that is consistent with the training data, but without being too specific
    - Underfitting: the hypothesis doesn't fit the training data well
    - Overfitting: the hypothesis tries too hard to fit the training data, preventing actual learning and generalization

- Bias vs. Variance

  - Remember, training data are subsamples drawn from the true distribution
  - Example: Studying Strategy
    - Study every chapter well => low variance and low bias
    - Study only a few chapters => high variance and low bias
    - Study every chapter roughly => low variance and high bias
    - Don't study => high variance and high bias

- Overfitting the Data

  - A classifier performing perfectly on the training data may not lead to the best generalized performance
    - There may be noise in the training data
    - The algorithm might be making decisions based on very little data
  - As the complexity of the hypothesis increases, the accuracy on the training data will continue to increase, but the generalized performance will eventually drop due to overfitting
  - Preventing Overfitting
    - Using a less expressive model
      - e.g., linear model
    - Adding regularization
      - Promote simpler models
    - Data perturbation (add noise in training)
      - Can be done algorithmically (e.g., dropout)
      - May increase the bias of the model
    - Stop the optimization process earlier
      - Sounds bad in theory, but works in practice

- How Do We Learn?

  - How can we find a good model from the hypothesis space?

- Linear Functions

  - Challenges
    - The hypothesis space contains an infinite number of functions
    - Several functions are consistent with the data
  - A possibility: local search
    - Start with a linear threshold function
    - See how well you're doing
    - Correct
    - Repeat until you converge
    - Optimize a function with calculus

- K-Nearest Neighbor

  - Motivation:

    - Spam
    - Learning from memorization

  - Nearest Neighbors: The Basic Version

    - Training examples are vectors `xi` associated with a label `yi`
      - e.g., `xi` = a feature vector for an email, `yi` = spam
    - Learning: just store all the training examples
    - Prediction: for a new example `x`
      - Find the training example `xi` that is closest to `x`
      - Predict the label of `x` to the label `yi` associated with `xi`

  - K-Nearest Neighbors

    - Training examples are vectors `xi` associated with a label `yi`

      - e.g., `xi` = a feature vector for an email, `yi` = spam

    - Learning: just store all the training examples

    - Prediction: for a new example `x`

      - Find the `k` closest training examples to `x`

    - Construct the label of `x` using these `k` points

    - Issue: how do we define distance?

      - How do we measure distances between instances in vector space?

        - Euclidean distance:

          - $$
            ||x_1-x_2||_2=\sqrt{\sum_{i=1}^n\left(x_{1,i}-x_{2,i}\right)^2}
            $$

        - Manhattan distance:

          - $$
            ||x_1-x_2||_1=\sum_{i=1}^n\left|x_{1,i}-x_{2,i}\right|
            $$

        - `Lp`-norm

          - Euclidean = `L2`, Manhattan = `L1`

          - $$
            ||x_1-x_2||_p=\left(\sum_{i=1}^n\left|x_{1,i}-x_{2,i}\right|^p\right)^{\frac{1}{p}},\quad p>0
            $$

      - In general, a good place to inject knowledge about the domain

      - Behavior of this approach can depend on this

  - Distance Between Instances

    - Most common distance is the Hamming distance
      - Number of bits that are different, or number of features that have a different value



## Lecture 4: K-Nearest Neighbor and the Curse of Dimensionality

- Learning Objectives:

  - KNN algorithms
  - Hyper-parameter tuning
    - Train/develop/test
    - `N`-fold cross-validation
  - Decision boundary
  - Curse of dimensionality
  - Practical concerns – data preprocessing

- KNN Algorithm

  - Training examples are vectors `xi` associated with a label `yi`
  - Learning: just store all the training examples
  - Prediction for a new example `x`:
    - Find the `k` closest training examples to `x`
    - Construct the label of `x` using these `k` points
  - Inductive Bias of KNN
    - Definition of inductive bias: the set of assumptions that the learner uses to predict outputs of unseen results
    - Label of point (data instance) is similar to the label of nearby points
    - Assumption may be broken in some cases, resulting to poor prediction

- Hyper-Parameters & Design Choices

  - Issue in designing KNN algorithm: how do we choose `k` and the distance measure?

  - Hyper-Parameters in KNN

    - Hyper-parameters:

      - Choosing `k` (number of nearest neighbors)

      - Distance measurement (e.g., `p` in the `Lp`-norm)

        - $$
          ||x_1-x_2||_p=\left(\sum_{i=1}^n\left|x_{1,i}-x_{2,i}\right|^p\right)^{\frac{1}{p}}
          $$

    - Those are not measured by the algorithm itself

      - Require empirical studies
      - The best parameter set is task/dataset-specific

  - Train/Dev/Test Splits

    - Split your data into Train/Dev/Test
      - Only use Train and Dev for developing models
    - Train: data for training models
      - Is generally the largest section
    - Dev (aka validation set): find the best parameters by evaluating models on dev
      - Like a practice exam, where Test is the final exam
    - Test: report the performance
      - Labels only revealed in test time
      - Shouldn't be modified often
    - Recipe of Train/Dev/Test
      - For each possible value of the hyper-parameter (e.g., `M = 1, 2, 3, ..., 10`)
        - Train a model using `D^TRAIN`
        - Evaluate the performance on `D^DEV`
      - Choose the model with the best performance on `D^DEV`
      - Optional: re-train the model on `D^TRAIN ∪ D^DEV ` with the best hyper-parameter set
      - Evaluate the final model on `D^TEST`
    - Trade-Off Between Train vs. Dev Size
      - Consider a situation where you have 120 data points and 20 data points are reserved for testing
        - What is the best way to split the remainder?
          - A) Train: 95, Dev: 5?
            - Result on Dev is not representative
          - B) Train: 60, Dev: 40?
            - Not enough data to train a model
    - `N`-Fold Cross Validation
      - Instead of a single Train-Dev split, split data into `N` equal-sized parts
        - Usually `N = 5`
        - Use each part as Dev once, with the rest as Train
      - Train and test `N` different classifiers
      - Report average accuracy and standard deviation of the accuracy
      - Finding Parameters Based on Cross Validation
        - Given `D^TRAIN` and `D^TEST`, for each possible value of the hyper-parameter (e.g., `K = 1, 2, 3, ..., 10`), conduct cross validation on `D^TRAIN` with parameter `K`
        - Choose the model with the best performance on `D^DEV`
        - Optional: re-train the model on `D^TRAIN ∪ D^DEV ` with the best hyper-parameter set
        - Evaluate the final model on `D^TEST`

  - Construct the Label of `x` Using These `k` Points

    - Majority vote
      - To break ties, it's better to use an odd-numbered `k`
    - Weighted vote
      - Weight by their distances; this is related to kernel methods (discussed later)

- Decision Boundary

  - The Decision Boundary for KNN
    - Is the K-nearest neighbors algorithm explicitly building a function?
      - No, it never forms an explicit hypothesis
    - Given a training set, what is the implicit function that is being computed?
    - Ex) If you have two training points, what will the decision boundary for 1-nearest neighbor be?
      - A line bisecting the two points
    - The Voronoi Diagram
      - For any point `x` in a training set `S`, the Voronoi Cell of `x` is a polytype consisting of all points closer to `x` than any other point in `S`
      - The Voronoi Diagram is the union of all Voronoi Cells
        - Covers the entire space

- Curse of Dimensionality

  - Ex) What fraction of the volume of a unit circle lies between radius `1 - ϵ` and radius `1`?

    - $$
      \frac{\pi\times1^2-\pi(1-\epsilon)^2}{\pi\times{1^2}}=1-(1-\epsilon)^2
      $$

  - Ex) What fraction of the volume of a unit sphere lies between radius `1 - ϵ` and radius `1`?

    - $$
      \frac{\frac{4\pi}{3}\times1^2-\frac{4\pi}{3}(1-\epsilon)^2}{\frac{4\pi}{3}\times{1^2}}=1-(1-\epsilon)^3
      $$

  - In `d` dimensions:

    - $$
      1-(1-\epsilon)^d
      $$

      - When `d` is large, this approaches `1`

    - In high dimensions, most of the volume is located far from the center

  - Most of the points in high dimensional space are far away from the origin

    - Need more data to "fill up the space"

  - Bad news for KNN in high-dimensional spaces

    - Even if most/all features are relevant, in high dimensional spaces, most points are equally far away from each other

  - Dealing with the Curse of Dimensionality

    - Most "real-world" data is not uniformly distributed in high dimensional space
      - Capturing the underlying dimensionality of the space – dimensionality reduction

- Data Preprocessing

  - Normalize data to have zero mean and unit standard deviation in each dimension

    - $$
      \bar{x}_d=\frac{1}{N}\sum_nx_{nd},\quad s_d^2=\frac{1}{N-1}\sum_n(x_{nd}-\bar{x}_d)^2
      $$

  - Scale the feature accordingly

    - $$
      x_{nd}\leftarrow\frac{x_{nd}-\bar{x}_d}{s_d}
      $$



## Lecture 5: Decision Trees and Linear Models

- Decision Trees

  - What is a Decision Tree?

    - A hierarchical data structure that represents data
    - Motivation: many decisions are tree structures
    - Terminology:
      - Root: the first point in the tree
      - Internal nodes: nodes in the middle, features used to categorize the data
      - Edge/branch: the different values of features that break the tree into subtrees
      - Leaf: the nodes representing outputs of the model
    - The Representation:
      - Decision trees are classifiers for instances represented as feature vectors
      - Nodes are tests for feature values
      - Edges: there is one branch for each value of the feature
      - Leaves specify the category (labels)
      - Can categorize instances into multiple disjoint categories
    - Handling Real-Valued Features
      - Usually, instances are represented as attribute-value pairs
      - Numerical values can be used by splitting nodes with thresholds
      - A tree partitions the feature space
    - Expressivity of Decision Trees
      - What Boolean functions can a decision tree represent?
        - Any Boolean function

  - Learning a Decision Tree

    - Basic Decision Trees Learning Algorithm

      - Data is processed in batch (i.e., all the data available)
      - Recursively build a decision tree top-down
      - Base case:
        - If all examples are labeled the same, return a single node with the label
      - Otherwise:
        - Pick an attribute and create branches
        - Split the tree

    - Decision Tree Algorithm: ID3

      - ```pseudocode
        ID3(S, Attributes, Label):
        	If all the attributes have the same label:
        		Return a single node with Label // Base case
        	A = attribute in Attributes that best classifies S
        	For each possible value v of A:
        		Add a new tree branch corresponding to A=v
        		Let Sv be the subset of examples in S with A=v
        		If Sv is empty:
        			Add leaf node with the common value of Label in S
          	Else:
          		Add the subtree ID3(Sv, Attributes - {A}, Label)
        ```

    - Which Attribute to Split?

      - The goal is to have the resulting decision tree as small as possible

        - Finding the minimal decision tree consistent with the data is NP-hard
        - We use a greedy heuristic search for a simple tree (cannot guarantee optimality)

      - How do we quantify it?

        - The most popular heuristic is based on the information gain

      - How do we Measure Information Gain?

        - Idea: gaining information reduces uncertainty
        - Uncertainty can be measured by entropy

      - Entropy

        - Entropy (impurity, disorder) of a set of examples, `S`, relative to a binary classification is:

        - $$
          H[S]=-P_+\log_2(P_+)-P_-\log_2(P_-)
          $$

          - Here, we define `0log0 = 0`

        - Where `P+` is the proportion of positive examples in `S` and `P-` is the proportion of negatives

        - Formal Definition:

          - If a random variable `S` has `K` different values, `a1, a2, ..., ak`, its entropy is given by:

            - $$
              H[S]=-\sum_{v=1}^KP(S=a_v)\log_2P(S=a_v)
              $$

      - Information Gain

        - The information gain of an attribute `a` is the expected reduction in entropy caused by partitioning on this attribute

        - $$
          Gain(S,A)=Entropy(S)-\sum_{v\in\text{Values}(A)}\frac{|S_v|}{|S|}Entropy(S_v)
          $$

          - `Sv` is the subset of `S` for which attribute `a` has value `v`
          - The entropy of partitioning the data is calculated by weighing the entropy of each partition by its size

  - Summary:

    - Representation: what are decision trees?
      - A hierarchical data structure that represents data
    - Algorithm: learning decision trees
      - The ID3 algorithm: a greedy heuristic
        - If all the examples have the same label, create a leaf with that label
        - Otherwise, find the "most informative" attribute and split the data for different values of that attribute
        - Recurse on the splits

- Linear Models

  - Recap: `X` as a vector space

    - `X` is an `N`-dimensional vector space (e.g., `R^N`)
      - Each dimension = one feature
    - Each `x` is a feature vector
    - Think of `x = [x1, ..., xN]` as a point in `X`
    - Goal is to find a hyperplane that separates the space

  - Hypothesis Space

    - Design an algorithm to find a good line within the hypothesis space

    - $$
      w^Tx+b=0
      $$

      - `w` and `b` are the parameters to represent a linear function



## Lecture 6: Linear Model and Perceptron

- What We Will Learn Today

  - Linear model

    - Basic linear algebra and linear classifier

    - Trick to remove bias term `b` in:

      - $$
        w^Tx+b=0
        $$

  - Perceptron algorithm

    - Perceptron update
    - Why it works
    - Convergence theorem – mistake bound

- Linear Model

  - Hypothesis Space

    - $$
      w=\begin{bmatrix}
      w_1\\
      w_2\\
      \vdots\\
      \vdots\\
      w_n
      \end{bmatrix},\quad
      x=\begin{bmatrix}
      x_1\\
      x_2\\
      \vdots\\
      \vdots\\
      x_n
      \end{bmatrix}\\
      w^Tx=w_1x_1+w_2x_2+\cdots+w_nx_n
      $$

    - `w^Tx` is the inner product (dot product) between `w` and `x`

    - In `n` dimensions, a linear classifier represents a hyperplane that separates the space into two half spaces

  - Linear Models for Binary Classification

    - Given training set:

      - $$
        D=\{(x,y)\},\quad x\in\mathbb{R}^d,y\in\{-1,+1\}
        $$

      - We use them to learn a hypothesis function `h ∈ H`:

        - $$
          H=\{h\ |\ h(x)=\text{sgn}(w^Tx+b)\}
          $$

          - $$
            \text{sgn}(x)=\begin{cases}
            1&\text{if }z\ge0\\
            -1&\text{otherwise}
            \end{cases}
            $$

          - Note: when `z = 0`, we can either assign `sgn(z) = 1` or `-1`

          - `w` and `n` are model/learnable parameters

        - Such that `y = h(x)`

    - Learn = Train = Find the best parameters `w`, `b`

  - Linear Classifiers

    - Linear classifiers classify an example `x` using the following classification process:

      - Use weighted sum of features (`w`) plus a bias (`b`) and apply the `sgn` function (sign)

    - A Simple Trick to Remove the Bias Term `b`

      - Can also consider an additional feature of weight `b` and value `1` to condense process into a single summation

      - $$
        \tilde{w}=\begin{bmatrix}
        w_1\\
        w_2\\
        \vdots\\
        \vdots\\
        w_n\\
        b
        \end{bmatrix},
        \tilde{x}=\begin{bmatrix}
        x_1\\
        x_2\\
        \vdots\\
        \vdots\\
        x_n\\
        b
        \end{bmatrix}
        $$

  - Learning a Linear Classifier

    - There are several algorithms/models:
      - Perceptron
      - Logistic regression
      - (Linear) support vector machines
      - Naive Bayes
      - etc.
    - Different methods define "best" in a different way

  - Linear Regression

    - Linear regression maps an example `x` into a real value `y`

    - Given training set:

      - $$
        D=\{(x,y)\},\quad x\in\mathbb{R}^d,y\in\{-1,+1\}
        $$

      - We use them to learn a hypothesis function `h ∈ H`:

        - $$
          H=\{h\ |\ h(x)=w^Tx+b\}
          $$

        - No `sgn` function

- Perceptron

  - Mistake + Correction = Learning

  - The Perceptron Algorithm

    - The goal is to find a separating hyperplane

    - An online algorithm

      - Processes one example at a time

    - ```pseudocode
      Initialize w ← 0 ∈ R^n
      For (x, y) in D:
      	y^hat = sgn(w^Tx)						// Predict
        if y&hat ≠ y, w ← w + yx 	// Update
      Return w
      ```

    - Prediction:

      - $$
        y^{test}\leftarrow\text{sgn}(\bold{w}^T\bold{x^{test}})
        $$

  - Intuition Behind Updates

    - Mistake on postive:

      - $$
        \bold{w_{t+1}}\leftarrow \bold{w_t}+\bold{x_i}
        $$

    - Mistake on negative:

      - $$
        \bold{w_{t+1}}\leftarrow \bold{w_t}-\bold{x_i}
        $$

    - Suppose we have made a mistake on a positive example

      - $$
        y=+1,\quad \bold{w_t}^T\bold{x}\le0
        $$

    - Call the new weight vector:

      - $$
        \bold{w_{t+1}}=\bold{w_t}+\bold{x}
        $$

    - The new dot product will be:

      - $$
        \bold{w_{t+1}}^T\bold{x}=(\bold{w_t}+\bold{x})^T\bold{x}=\bold{w_t}^T\bold{x}+\bold{x}^T\bold{}
        $$

    - For a positive example, the Perceptron update will increase the score assigned to the same input

    - Similar reasoning for negative examples

  - Perceptron Learnability

    - Perceptron cannot learn what it cannot represent – only linearly separable functions
      - e.g., parity functions (like XOR) cannot be learned

  - Convergence Theorem

    - If there exists a set of weights that are consistent with the data (i.e., the data is linearly separable), the Perceptron algorithm will converge
      - The update stops after making a finite number of mistakes
      - The convergence rate depends on the difficulty of the problem
    - Note: this is the condition of the data, we may not know what the hyperplane is
    - If the data is not linearly separable, then the algorithm will eventually repeat the same set of weights and enter an infinite loop

  - Margin

    - If a hyperplane can separate the data, the margin of a hyperplane for the dataset is the distance between the hyperplane and the data point closest to it

    - The margin of a dataset (`γ`) is the maximum margin possible for that dataset using any weight vector

    - Let `{(x1, y1), (x2, y2), ..., (xm, ym)}` be a set of training data, if for all data points `(xi, yi)` in the training set, there exists a unit vector `u` such that:

      - $$
        y_i(\bold{u}^T\bold{x_i})\ge\gamma_i
        $$

    - Margin is not scale invariant

      - i.e., if we double the size of every input `xi`, the margin `γ` is also doubled

      - The "difficulty" of the problem can be captured by:

        - $$
          \frac{R}{\gamma},\quad||x_i||\le R,\forall_i
          $$

  - Mistake Bound Theorem

    - Let `{(x1, y1), (x2, y2), ..., (xm, ym)}` be a sequence of training examples such that for all `i`, the feature vector `xi ∈ R^n` `||xi|| ≤ R`, and the label `y ∈ {-1, +1}`

    - Suppose there exists some unit vector `u ∈ R^n` such that for some `γ > 0`, we have:

      - $$
        y_i(\bold{u}^T\bold{x_i})\ge\gamma_i
        $$

    - Then, the number of mistakes the Perceptron algorithm makes on the training set is bounded by:

      - $$
        \left(\frac{R}{\gamma}\right)^2
        $$

  - Beyond the Separable Case

    - Good news: Perceptron makes no assumption about data distribution, could even be adversarial
    - Bad news: Real-world data are often no linearly separable



## Lecture 7: Logistic Regression, Sigmoid Functions, and Gradient Descent

- Recall: The Geometry of a Linear Classifier

  - If we use a unit normal vector `u` to represent the hyperplane, the distance between point `x` to the plane is `|u^Tx + b|` or `y(u^Tx + b)`

    - Hold as long as the model classifies correctly

  - If the distance between the closest point in dataset `D` to the plane `u` is `γ`:

    - $$
      y_i(\bold{u}^Tx_i+b)\ge\gamma,\forall(x_i,y_i)\in D
      $$

    - As long as the dataset is linearly separable

- Logistic Regression

  - What You Will Learn Today

    - Logistic regression assumption
    - Sigmoid function
    - Maximum likelihood principle
    - Optimization in ML
      - Stochastic gradient descent

  - What Makes Data Not Linearly Separable?

    - Decision boundary is nonlinear
      - e.g., XOR
    - Noise in the training data
      - Outlier due to annotation errors
    - Not enough features
    - The nature of the prediction task
      - 

  - Classification, but...

    - The output `y`y is a discrete value
    - Instead of predicting the output label, let's predict `P(y = 1 | x)`
      - How likely a given label is accurate if the feature vector lies in this region
    - Perceptron doesn't produce probability estimates

  - Predict `P(y = 1 | x)`

    - Input:

      - $$
        x\in\mathbb{R}^d
        $$

    - Output:

      - $$
        y\in\{-1,1\}
        $$

    - Build a model `h(x)` such that:

      - $$
        h(\bold{x})=\sigma(\bold{w}^T\bold{x}+b)\approx P(y=1\ |\ \bold{x})
        $$

        - A regression problem

        - `σ` is a sigmoid function:

          - $$
            \begin{equation*}
            \begin{split}
            \sigma(z)&=\frac{\text{exp}(z)}{1+\text{exp}(z)}\\
            & = \frac{1}{1+\text{exp}(-z)}
            \end{split}
            \end{equation*}
            $$

  - How Can We Design Such a Transformation Function?

    - Idea 1: Function always outputs a positive value

      - `exp()` is always positive
      - `exp(w^Tx + b)` always returns a positive value
      - Problem: the function may return a value `>1`

    - Idea 2: Normalize the value such that it is less than `1`

      - `exp(w^Tx + b) ∈ (0, ∞)` grows very fast, so we need to us `exp` to normalize itself

      - Let's use:

        - $$
          \sigma(\bold{w}^T\bold{x}+b)=\frac{\text{exp}(\bold{w}^T\bold{x}+b)}{1+\text{exp}(\bold{w}^T\bold{x}+b)}
          $$

      - $$
        \bold{w}^T\bold{x}+b\rightarrow\infty\Rightarrow\sigma(\bold{w}^T\bold{x}+b)\rightarrow1\\
        \bold{w}^T\bold{x}+b\rightarrow-\infty\Rightarrow\sigma(\bold{w}^T\bold{x}+b)\rightarrow0\\
        $$

  - The Sigmoid Function

    - The `σ(z)` function is called a sigmoid function (or logistic function)

    - $$
      \begin{equation*}
      \begin{split}
      \sigma(z)&=\frac{\text{exp}(z)}{1+\text{exp}(z)}\\
      & = \frac{1}{1+\text{exp}(-z)}
      \end{split}
      \end{equation*}
      $$

  - Summary (Modeling)

    - What is the goal of logistic regression?

      - Model `P(y = 1 |  x)`

    - What is the hypothesis space?

      - $$
        H=\{h\ |\ h:X\rightarrow P(Y\ |\ X),h(\bold{x})=\sigma(\bold{w}^T\bold{x}+b)\}\\
        \sigma(z)=\frac{1}{1+\text{exp(z)}}
        $$

    - We want to find `h(x)` such that:

      - $$
        h(\bold{x})\approx P(y=1\ |\ \bold{x})
        $$

- Decision Boundary of Logistic Regression

  - Predicting a Label

    - $$
      P(y=1\ |\ \bold{x};\bold{w})=\sigma(\bold{w}^T\bold{x})=\frac{1}{1+\text{exp}(-\bold{w}^T\bold{x})}
      $$

    - Compute `σ(w^Tx)`

      - If this is greater than `1/2`, predict `1`, otherwise predict `-1`

- How to Train a Logistic Regression Model

  - Logistic Regression: Setup

    - The Setting

      - Binary classification
      - Inputs: feature vectors `x ∈ R^N`
      - Labels: `y ∈ {-1, +1}`

    - Training Data

      - `S = {(x, y)}`, `m` examples

    - Hypothesis Space

      - $$
        H=\{h\ |\ h:X\rightarrow P(Y\ |\ X),h(\bold{x})=\sigma(\bold{w}^T\bold{x}+b)\}\\
        \sigma(z)=\frac{1}{1+\text{exp(z)}}
        $$

    - Learning Goal

      - Find an `h ∈ H`, such that `h(x) ≈ P(y = 1 | x)`
      - How?

  - Maximum Likelihood

    - Drawing Color Cards from the Envelope

      - Sample with replacement `n` times

      - `k` times we get a purple card and `n - k` times, we get a yellow card

      - The joint probability (likelihood)

        - $$
          C^n_k\theta^k(1-\theta)^{n-k}
          $$

      - What is the best `θ` that maximizes the joint probability?

      - Solving:

        - $$
          \text{max}_\theta\theta^k(1-\theta)^{n-k}
          $$

      - Equivalently, we can solve:

        - $$
          \text{max}_\theta\log(\theta^k(1-\theta)^{n-k})=\text{max}_\theta k\log\theta+(n-k)\log(1-\theta)
          $$

      - At the optimum:

        - $$
          \frac{\text{d}(k\log\theta+(n-k)\log(1-\theta))}{d\theta}=0\\
          \frac{k}{\theta}-\frac{n-k}{1-\theta}=0\\
          \theta(n-k)=(1-\theta)k\\
          \theta=\frac{k}{n}
          $$

    - Maximum Likelihood Estimator (Formal Definition)

      - Likelihood Function of Parameters

        - Let `X1, ..., XN` be IID with PDF `p(x | θ)` => the likelihood function is defined by `L(θ)`:

          - $$
            L(\theta)=p(X_1,\ldots,X_N;\theta)=\prod_{i=1}^Np(X_i;\theta)
            $$

          - Note: the likelihood function is just the joint density of the data, except that we treat it as a function of the parameter `θ`

        - Definition: the maximum likelihood estimator (MLE) `θ^hat` is the value of `θ` that maximizes `L(θ)`

          - The log-likelihood function is defined by `l(θ) = log L(θ)`
          - Its maximum occurs at the same places as that of the likelihood function

      - Maximum Likelihood Estimator for Logistic Regression

        - $$
          \text{argmax}_{w,b}P(S;w,b)=\text{argmax}_{w,b}\prod_{i=1}^mP(y_i\ |\ x_i;w,b)
          $$

        - Equivalent to solve:

          - $$
            \text{argmax}_{w,b}\sum_{i=1}^m\log P(y_i\ |\ x_i;w,b)
            $$

        - Remember our assumption:

          - $$
            P(y=1\ |\ x;w,b)=\sigma(w^Tx+b)=\frac{1}{1+\text{exp}(-(w^Tx+b))}\\
            \begin{equation*}
            \begin{split}
            P(y=-1\ |\ x;w,b)&=1-\sigma(w^Tx+b)\\
            &=1-\frac{1}{1+\text{exp}(-(w^Tx+b))}\\
            &=\frac{\text{exp}(-(w^Tx+b))}{1+\text{exp}(-(w^Tx+b))}\\
            &=\frac{1}{1+\text{exp}(w^Tx+b)}\\
            &=\sigma(-(w^Tx+b))
            \end{split}
            \end{equation*}
            $$

          - $$
            \begin{equation*}
            \begin{split}
            P(y_i\ |\ x_i;w,b)&=\begin{cases}
            \sigma(w^Tx_i+b)\\
            \sigma(-(w^Tx_i+b))
            \end{cases}\\
            &=\sigma(y_i(w^Tx_i+b))
            \end{split}
            \end{equation*}
            $$

          - $$
            \text{argmax}_{w,b}\sum_{i=1}^m\log\sigma(y_i(w^Tx_i+b))=\text{argmax}_{w,b}-\sum_{i=1}^m\log(1+\text{exp}(-y_i(w^Tx_i+b)))
            $$

- How to Optimize the Loss

  - How to Minimize the Loss

    - Optimization methods:
      - Gradient descent
      - Stochastic gradient descent
        - Good enough to give a good model + good performance
      - Analytic solution
      - Many other approaches

  - How to Solve It

    - $$
      \text{argmax}_{w,b}-\sum_{i=1}^m\log(1+\text{exp}(-y_i(w^Tx_i+b)))
      $$

    - There is no closed-form solution

    - Max `f(x)` is equivalent to min `-f(x)`

      - Only need to consider minimization problems

    - One way to solve it is by gradient descent

- Gradient Descent

  - ```pseudocode
    Start at a random point
    Repeat
    	Determine a descent direction
    	Choose a step size
    	Update
    Until stopping criterion is satisfied
    ```

  - Where Will We Converge?

    - If the function is convex, it converges to the global optimum (need proper choice of step size)

    



## Lecture 8: Neural Network and Deep Learning

- What You Will Learn Today
  - Optimization
    - Gradient descent
    - Stochastic gradient descent (SGD)

  - Evaluation Metrics
  - Neural Network/


